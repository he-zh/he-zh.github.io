<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Why testing conditional independence is so hard? | Zheng HE</title> <meta name="author" content="Zheng HE"/> <meta name="description" content="Academic website of Zheng HE. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/H.png?1388fb2ffcaa33939e29e300e7ac9694"/> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://he-zh.github.io/blog/2025/ci-hardness/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Why testing conditional independence is so hard?",
      "description": "",
      "published": "December 10, 2025",
      "authors": [
        {
          "author": "Zheng HE",
          "authorURL": "",
          "affiliations": [
            {
              "name": "University of British Columbia",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-navsticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Zheng&nbsp;</span>HE</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Why testing conditional independence is so hard?</h1> <p></p> </d-title><d-byline></d-byline><div class="d-contents-sticky" style="opacity: 0;"> <nav class="figcaption"> <h3>Contents</h3> <div><a href="#background">Background</a></div> <div><a href="#why-ci-testing-is-fundamentally-hard">Why CI testing is fundamentally hard</a></div> <div><a href="#why-it-still-fails-in-practice">Why it still fails in practice</a></div> <div><a href="#type-i-error-inflation-and-type-i-ii-tradeoff">Type I error inflation and type I/II tradeoff</a></div> <div><a href="#final-summary">Final Summary</a></div> </nav> </div> <script>!function(){function e(){var e=document.querySelector("d-article");if(e){var n=e.getBoundingClientRect().top;t.style.opacity=n<80?"1":"0"}}var t=document.querySelector(".d-contents-sticky");t&&(window.addEventListener("scroll",e,{passive:!0}),e())}();</script> <d-article> <p>Conditional independence (CI) testing is widely used in causal discovery, scientific modeling, fairness, domain generalization, and robustness analysis. And yet it often fails in practice.</p> <p>Why?</p> <p>Let’s unpack the story.</p> <h2 id="background">Background</h2> <p>Before we talk about hardness, we need to understand what conditional independence really means — and how we try to measure it.</p> <h3 id="conditional-independence">Conditional independence</h3> <p>What does conditional independence between \(A\) and \(B\) given \(C\) mean?</p> <p>At a high level:</p> <blockquote> <p>Once we account for \(C\), does \(B\) still tell us anything about \(A\)?</p> </blockquote> <p>Two concrete examples help build intuition.</p> <h3 id="example-1-fairness-in-lending">Example 1: fairness in lending</h3> <ul> <li>\(A\) = loan decision</li> <li>\(B\) = race</li> <li>\(C\) = credit score, income</li> </ul> <p>Race and loan decisions may be correlated marginally.<br/> But once we control for creditworthiness, race <em>should</em> no longer influence the decision.</p> <p>If it still does, the system may be unfair.</p> <h3 id="example-2-distribution-shift">Example 2: distribution shift</h3> <ul> <li>\(A\) = model prediction</li> <li>\(B\) = time of day</li> <li>\(C\) = true location</li> </ul> <p>A model might use “time of day” as a shortcut for predicting location.<br/> If so, its predictions will fail when time distributions shift.</p> <p>If predictions remain independent of time once we condition on true location, the model is robust.</p> <h3 id="formal-definition">Formal definition</h3> <p>Conditional independence is defined as</p> \[A \perp\!\!\!\perp B \mid C \quad \Longleftrightarrow \quad P_{A,B \mid C} = P_{A \mid C} P_{B \mid C}.\] <blockquote> <p>Once \(C\) is fixed, knowing \(B\) provides no additional information about \(A\).</p> </blockquote> <p>Equivalently, for almost every value of \(C\),</p> \[\text{Cov}(f(A), g(B) \mid C) = 0\] <p>for all measurable functions \(f\) and \(g\).</p> <p>This functional view turns out to be crucial.</p> <h3 id="ci-as-a-hypothesis-testing-problem">CI as a hypothesis testing problem</h3> <p>At its core, hypothesis testing asks:</p> <ul> <li><strong>Null hypothesis (\(H_0\)):</strong> nothing interesting is happening.</li> <li><strong>Alternative hypothesis (\(H_1\)):</strong> something is going on.</li> </ul> <p>In CI testing:</p> \[H_0: A \perp\!\!\!\perp B \mid C\] \[H_1: A \not\!\perp\!\!\!\perp B \mid C\] <p>We compute a statistic from data. If it is too extreme, we reject \(H_0\).</p> <p>Two types of error can occur:</p> <ul> <li> <p><strong>Type I error (false positive):</strong><br/> Rejecting \(H_0\) when conditional independence actually holds.<br/> In CI testing, this means concluding that \(A\) and \(B\) are conditionally dependent when they are not.</p> </li> <li> <p><strong>Type II error (false negative):</strong><br/> Failing to reject \(H_0\) when conditional dependence actually exists.<br/> In CI testing, this means missing real conditional structure.</p> </li> </ul> <p>A good test aims to:</p> <ul> <li>Control Type I error at a predefined level \(\alpha\).</li> <li>Minimize Type II error (maximize power).</li> </ul> <p>For many classical testing problems, this tradeoff is manageable.</p> <p>For conditional independence, both errors are unusually difficult to control simultaneously.</p> <h3 id="measuring-conditional-independence">Measuring conditional independence</h3> <p>A characterization of conditional independence is: \(A \perp\!\!\!\perp B \mid C\) if and only if, for all square-integrable functions \(f \in L_A^2\), \(g \in L_B^2\), \(w \in L_C^2\),</p> \[\mathbb{E}_{C}\Big[ w(C) \, \mathbb{E}_{AB|C}\Big[ \big(f(A)- \mathbb{E}[f(A)|C]\big) \big(g(B)- \mathbb{E}[g(B)|C]\big) \Big] \Big] = 0.\] <p>Let’s unpack this.</p> <ul> <li> <p>First, we measure conditional covariance by \(f(A) - \mathbb{E}[f(A)|C]\) and \(g(B) - \mathbb{E}[g(B)|C]\)</p> </li> <li> <p>Then we weight conditional covariance by \(w(C)\) to emphasize specific regions of \(C\).</p> </li> </ul> <p>If any conditional covariance remains on a region of \(C\) with non-negligible probability, an appropriate \(w(C)\) will detect it.</p> <p>So conditional independence means:</p> <blockquote> <p>No residual dependence remains after removing the effect of \(C\).</p> </blockquote> <h3 id="rkhs-view-from-functions-to-operators">RKHS view: from functions to operators</h3> <p>It is impossible to test all square-integrable functions, thus we use functions in a Reproducing Kernel Hilbert Space (RKHS).</p> <p>An RKHS \(\mathcal{H}_A\) contains functions that are linear w.r.t. \(\phi_A(a)\)</p> \[f(a) = \langle w, \phi_A(a) \rangle,\] <p>where \(\phi_A\) is a feature map.</p> <p>Define the conditional mean embedding</p> \[\mu_{A|C}(c) = \mathbb{E}[ \phi_A(A) \mid C = c ].\] <p>It satisfies</p> \[\langle \mu_{A|C}(c), f \rangle_{\mathcal{H}_A} = \mathbb{E}[ f(A) \mid C = c ].\] <p>So conditional expectations become inner products in Hilbert space.</p> <p>We define the conditional cross-covariance operator</p> \[\mathcal{C}_{AB|C}(c) = \mathbb{E}_{AB|C}\Big[ \big(\phi_A(A)-\mu_{A|C}(c)\big) \otimes \big(\phi_B(B)-\mu_{B|C}(c)\big) \mid C=c \Big].\] <p>This operator satisfies</p> \[\langle f \otimes g, \mathcal{C}_{AB|C}(c) \rangle = \text{Cov}(f(A), g(B) \mid C=c).\] <p>So it encodes <em>all</em> conditional covariances.</p> <h3 id="the-kci-operator">The KCI operator</h3> <p>To aggregate over \(C\), we define</p> \[\mathcal{C}_{\text{KCI}} = \mathbb{E}_C\Big[ \mathcal{C}_{AB|C}(C) \otimes \phi_C(C) \Big].\] <p>For any test functions \(f, g, w\),</p> \[\langle f \otimes g, \mathcal{C}_{\text{KCI}}\, w \rangle = \mathbb{E}_C\Big[ w(C) \, \mathbb{E}_{AB|C} \Big[ (f(A)-\mathbb{E}[f(A)|C]) (g(B)-\mathbb{E}[g(B)|C]) \Big] \Big].\] <p>If the RKHSs \(\mathcal{H}_A, \mathcal{H}_B, \mathcal{H}_C\) are sufficiently rich (e.g., \(L^2\)-universal), then</p> \[\mathcal{C}_{\text{KCI}} = 0 \quad \Longleftrightarrow \quad A \perp\!\!\!\perp B \mid C.\] <p>A common test statistic <d-cite key="zhang2011kernel"></d-cite> is</p> \[\text{KCI} = \|\mathcal{C}_{\text{KCI}}\|_{\text{HS}}^2.\] <p>So the problem reduces to:</p> <blockquote> <p>Estimate this operator from finite samples and determine whether it is zero.</p> </blockquote> <p>That is where the real difficulty begins.</p> <h2 id="why-ci-testing-is-fundamentally-hard">Why CI testing is fundamentally hard</h2> <h3 id="the-binary-embedding-trick">The binary embedding trick</h3> <p>Now comes the key construction.</p> <ol> <li>Sample scalars \(A, B, C\) where \(A \not\!\perp\!\!\!\perp B \mid C\)</li> <li>Take binary expansions of \(A, B, C\).</li> <li>Truncate each to 100 bits:</li> </ol> \[A_{100}, \quad B_{100}, \quad C_{100}.\] <ol> <li>Embed \(A_{100}\) into \(C_{100}\) by concatenation.</li> </ol> <p>For example:</p> <ul> <li> \[C_{100} = 10011001\dots\] </li> <li> \[A_{100} = 10111100\dots\] </li> </ul> <p>Define new \(C\) as:</p> \[C_\text{new} = (C_{100} || A_{100}) = 10011001...10111100...\] <p>Now every bit of \(A_{100}\) is encoded inside \(C_\text{new}\).</p> <p>Finally, add tiny noise to make the distribution continuous.</p> <h3 id="what-just-happened">What just happened?</h3> <p>After this construction:</p> <ul> <li>\(A_\text{new}\) can be reconstructed from \(C_\text{new}\)</li> <li>So conditioning on \(C_\text{new}\) removes all information \(B_\text{new}\) has about \(A_\text{new}\)</li> </ul> <p>Therefore:</p> \[A_\text{new} \perp\!\!\!\perp B_\text{new} \mid C_\text{new}.\] <p>This means we can construct conditionally independent distribution which is arbitrarily close to the conditional dependent distribution.</p> <p>But here’s the catch:</p> <p>The independence holds only because of information hidden in extremely fine digits of \(C\).</p> <p>To detect it, a test would need infinite precision.</p> <p>No finite sample test can reliably examine those tail bits.</p> <h3 id="the-shahpeters-impossibility-theorem">The Shah–Peters impossibility theorem</h3> <p>This construction is not just a clever trick. It reflects a deep structural limitation formalized by Shah and Peters (2020)<d-cite key="shah2018hardness"></d-cite>.</p> <blockquote> <p>For any finite-sample conditional independence test, and for any alternative distribution where \(A \not\!\perp\!\!\!\perp B \mid C\), there exists a null distribution (where \(A \perp\!\!\!\perp B \mid C\)) that the test cannot reliably distinguish from that alternative.</p> </blockquote> <p>More concretely:</p> <ul> <li>If your test has power strictly greater than \(\alpha\) against some alternative,</li> <li>Then there must exist at least one null distribution under which the test’s Type I error exceeds \(\alpha\).</li> </ul> <p>In other words, no CI test can be uniformly valid over all continuous distributions.</p> <p>There is no procedure that simultaneously:</p> <ul> <li>Controls Type I error at level \(\alpha\) for all nulls, and</li> <li>Achieves nontrivial power against all alternatives.</li> </ul> <p>This is not a shortcoming of current algorithms.</p> <p>It is a fundamental limitation of the problem itself.</p> <h2 id="why-it-still-fails-in-practice">Why it still fails in practice</h2> <p>You might think: these are adversarial constructions, surely in practice we don’t encounter them.</p> <p>Correct — we rarely face carefully engineered binary embedding tricks.</p> <p>But the <em>mechanism</em> behind the impossibility result is not artificial.</p> <p>The core issue is this:</p> <blockquote> <p>Conditional dependence can live in structured, localized, or oscillatory features of the distribution.</p> </blockquote> <p>And detecting those features from finite samples is fundamentally delicate.</p> <p>Let’s see how this shows up in a realistic setting.</p> <h3 id="a-realistic-example">A realistic example</h3> <p>Consider</p> \[A = f_A(C) + r_A\] \[B = f_B(C) + r_B\] <p>where</p> \[(r_A, r_B) \mid C \sim \mathcal{N} \left( 0, \begin{pmatrix} 1 &amp; \gamma(C) \\ \gamma(C) &amp; 1 \end{pmatrix} \right).\] <p>Here:</p> <ul> <li>\(f_A\) and \(f_B\) capture the systematic effect of \(C\).</li> <li>\(r_A\) and \(r_B\) represent unexplained noise.</li> <li>The only possible dependence between \(A\) and \(B\) comes from the residual correlation \(\gamma(C).\)</li> </ul> <p>Now define:</p> <ul> <li>Under \(H_0\): \(\gamma(C) = 0\)</li> <li>Under \(H_1\): \(\gamma(C) = \sin(C)\)</li> </ul> <p>So under the alternative, the residual correlation oscillates smoothly as a function of \(C\).</p> <p>Why this is subtle?</p> <p>Notice something important.</p> <p>The <em>marginal</em> residual correlation is</p> \[\mathbb{E}[\gamma(C)] = \mathbb{E}[\sin(C)].\] <p>Since \(C \sim \mathcal{N}(0,1)\) and \(\sin(\cdot)\) is symmetric,</p> \[\mathbb{E}[\sin(C)] = 0.\] <p>So globally, the residual correlation averages out.</p> <p>Marginally, there is no detectable correlation.</p> <p>The dependence only appears <em>locally</em> in regions of \(C\).</p> <div class="row justify-content-center mt-3"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/hardness/residuals_L.pdf-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/hardness/residuals_L.pdf-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/hardness/residuals_L.pdf-1400.webp"/> <img src="/assets/img/blog/hardness/residuals_L.pdf" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Marginal and conditional covariance of A and B given C. </div> <p>This is the key difficulty:</p> <blockquote> <p>Conditional dependence may oscillate, cancel globally, and only be visible at the right scale.</p> </blockquote> <ul> <li>If a test averages too aggressively over \(C\), it will miss this structure entirely.</li> <li>If it localizes too aggressively, estimates become very unstable and noisy.</li> </ul> <p>This tension already makes detecting subtle dependence fragile.</p> <p>But CI testing is hard for a deeper reason.</p> <p>It is not only difficult to detect true conditional dependence — it is also dangerously easy to detect dependence that is not actually there.</p> <p>If we look at the wrong structure (for example, use the wrong kernel scale), we may smooth away real dependence and fail to detect it.</p> <p>If the conditional means are estimated inaccurately, we may introduce artificial residual correlation and falsely conclude that dependence exists.</p> <p>In short:</p> <blockquote> <p>The same procedure can both overlook real dependence and hallucinate spurious dependence.</p> </blockquote> <p>That dual instability is what makes conditional independence testing fundamentally delicate.</p> <h3 id="when-conditional-means-are-perfect">When conditional means are perfect</h3> <p>To understand where things go wrong in practice, let’s first look at the idealized setting — where we know the conditional means exactly.</p> <p>When linear kernels are used for variables A and B, i.e., \(\phi_A(A)=A\) and \(\phi_B(B)=B\), the Kernel-based Conditional Independence (KCI) can be understood in three conceptual steps.</p> <ol> <li>Get the perfect conditional means: \(\mu_{A|C}(c) = \mathbb{E}[A \mid C=c], \qquad \mu_{B|C}(c) = \mathbb{E}[B \mid C=c].\)</li> <li>Form residuals by removing the effect of \(C\): \(R_A = A - \mu_{A|C}(C), \qquad R_B = B - \mu_{B|C}(C).\)</li> <li>Measure whether the residuals are still dependent, using a kernel on \(C\) to localize the comparison across different regions of the conditioning variable.</li> </ol> <p>Intuitively, KCI asks: after removing everything that can be explained by \(C\), is there any remaining dependence between \(A\) and \(B\)?</p> <p>In this ideal infinite-sample regime, where the conditional means are known perfectly:</p> <ul> <li> <p>Under \(H_0\), \(\text{Cov}(R_A, R_B \mid C) = 0\) almost surely, so the KCI statistic converges to zero.</p> </li> <li> <p>Under \(H_1\), residual dependence remains for some values of \(C\), and the kernel aggregation detects it.</p> </li> </ul> <p>In this idealized setting:</p> <ul> <li>Type I error is controlled, because under the null the population statistic is exactly zero.</li> <li>Type II error depends only on how subtle the remaining dependence is, and whether the chosen kernel can resolve the relevant structure.</li> </ul> <p>In other words, if the conditional means were known exactly, CI testing would be a well-behaved problem. The only difficulty would be statistical power: do we have enough data and the right resolution to detect the existing dependence?</p> <p>The real trouble begins when we have to estimate those conditional means from data.</p> <h3 id="when-we-have-to-estimate-the-conditional-means">When we have to estimate the conditional means</h3> <p>Everything above assumed we knew the true conditional means</p> <p>In practice, we do not.</p> <p>We estimate them from data, typically using kernel ridge regression <d-cite key="pogodin2024splitkci"></d-cite>.</p> <p>Write the estimators as</p> \[\hat{\mu}_{A|C}(c) = \mu_{A|C}(c) + \delta_{A|C}(c),\] \[\hat{\mu}_{B|C}(c) = \mu_{B|C}(c) + \delta_{B|C}(c),\] <p>where \(\delta_{A\mid C}\) and \(\delta_{B\mid C}\) are the regression errors.</p> <p>What happens to the residuals?</p> <p>The empirical residuals are now</p> \[\hat{R}_A = A - \hat{\mu}_{A|C}(C), \qquad \hat{R}_B = B - \hat{\mu}_{B|C}(C).\] <p>Substituting,</p> \[\hat{R}_A = (A - \mu_{A|C}(C)) - \delta_{A|C}(C),\] \[\hat{R}_B = (B - \mu_{B|C}(C)) - \delta_{B|C}(C).\] <p>Under our generative model,</p> \[A - \mu_{A|C}(C) = r_A, \qquad B - \mu_{B|C}(C) = r_B.\] <p>So</p> \[\hat{R}_A = r_A - \delta_{A|C}(C), \qquad \hat{R}_B = r_B - \delta_{B|C}(C).\] <p>Now multiply:</p> \[\hat{R}_A \hat{R}_B = r_A r_B - r_A \delta_{B|C}(C) - r_B \delta_{A|C}(C) + \delta_{A|C}(C)\delta_{B|C}(C).\] <p>Taking conditional expectation given \(C\) (and assuming regression is trained on independent data so errors are fixed w.r.t. test sample),</p> \[\mathbb{E}[\hat{R}_A \hat{R}_B \mid C] = \mathbb{E}[r_A r_B \mid C] + \delta_{A|C}(C)\delta_{B|C}(C).\] <p>But</p> \[\mathbb{E}[r_A r_B \mid C] = \gamma(C).\] <p>So we obtain</p> \[\mathbb{E}[\hat{R}_A \hat{R}_B \mid C] = \gamma(C) + \delta_{A|C}(C)\delta_{B|C}(C).\] <p><strong>Under \(H_0\),</strong> \(\gamma(C) = 0.\) So the population residual covariance becomes</p> \[\mathbb{E}[\hat{R}_A \hat{R}_B \mid C] = \delta_{A\mid C}(C)\delta_{B\mid C}(C).\] <p>Even though \(A \perp B \mid C\) holds in truth.</p> <p>When KCI aggregates over \(C\) using the kernel,</p> \[\text{KCI} = \mathbb{E} \left[ k_C(C,C') \, \delta_{A\mid C}(C) \delta_{A\mid C}(C') \delta_{B\mid C}(C) \delta_{B\mid C}(C') \right].\] <p>So the regression errors induce a nonzero population statistic.</p> <p>This is not sampling noise.</p> <p>It is structural bias introduced by imperfect regression.</p> <h2 id="type-i-error-inflation-and-type-iii-tradeoff">Type I error inflation and type I/II tradeoff</h2> <h3 id="why-type-i-error-explodes">Why type I error explodes</h3> <p>Under ideal conditions (perfect regression), the KCI statistic behaves like a degenerate U-statistic under the null.</p> <p>Because the population statistic is exactly zero under \(H_0\), the first-order term of the U-statistic vanishes. This degeneracy forces the variance to decay at rate \(1/n\).</p> <p>Null approximations — whether chi-square mixtures, Gamma approximations, or wild bootstrap — rely critically on this structure.</p> <p>They assume:</p> <ul> <li>The population mean under \(H_0\) is zero.</li> <li>The statistic is asymptotically centered.</li> <li>The dominant stochastic fluctuation is of order \(1/n\) around zero.</li> </ul> <p>When conditional means are estimated imperfectly.</p> <p>Under \(H_0\), the statistic no longer has zero population mean:</p> <p>Because once the statistic has nonzero mean, the U-statistic is no longer degenerate. Its leading term behaves like an empirical average of nonzero quantities.</p> <p>Formally:</p> <ul> <li>The bias term \(\mathbb{E}[\delta_{A\mid C}(C)\delta_{B\mid C}(C')]\) does not vanish with \(n\).</li> <li>The centered fluctuations around this bias are of order \(1/\sqrt{n}\).</li> </ul> <p>So instead of shrinking toward zero, the statistic concentrates around a positive constant:</p> \[\text{KCI}_n = \text{Bias} + O_p(1/\sqrt{n}).\] <p>Unless the regression error itself shrinks sufficiently fast, the bias remains.</p> <table> <thead> <tr> <th>Case</th> <th>Mean of KCI</th> <th>Std Dev</th> </tr> </thead> <tbody> <tr> <td>Perfect regression</td> <td>0</td> <td>\(O(1/n)\)</td> </tr> </tbody> <tbody> <tr> <td>Imperfect regression</td> <td>\(O(1)\)</td> <td>\(O(1/\sqrt{n})\)</td> </tr> </tbody> </table> <p><strong>The consequence:</strong></p> <p>Null calibration procedures still assume a centered statistic.</p> <p>But the true distribution is shifted.</p> <p>As test sample size \(n\) grows:</p> <ul> <li>The variance shrinks.</li> <li>The bias does not.</li> </ul> <p>Eventually, the statistic will almost surely exceed any fixed null threshold.</p> <p>Type I error thus inflate with increasing \(n\).</p> <p>This is why regression error is not a small nuisance.</p> <p>It fundamentally changes the asymptotic regime.</p> <h3 id="type-i-and-type-ii-error-tradeoff">Type I and type II error tradeoff</h3> <p>In principle, we choose the kernel (especially the bandwidth on \(C\)) to maximize power — that is, to better detect conditional dependence.</p> <p>But here is the subtle danger:</p> <blockquote> <p>The same kernel choice that amplifies true dependence can also amplify regression-induced bias.</p> </blockquote> <p>Recall that under imperfect regression, the null statistic contains the term</p> \[\delta_{A|C}(C)\,\delta_{B|C}(C).\] <p>These regression errors are not arbitrary noise. They are smooth, structured functions of \(C\).</p> <p>When we optimize the kernel bandwidth to increase sensitivity to dependence, we are effectively choosing a weighting function over \(C\).</p> <p>If that weighting aligns with regions where \(\delta_{A|C}(C)\,\delta_{B|C}(C)\) is large, the test statistic increases — even under the null.</p> <p>In other words:</p> <ul> <li>When we tune the kernel to detect real structure,</li> <li>We may instead be tuning it to highlight structured regression error.</li> </ul> <div class="row justify-content-center mt-3"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/hardness/type1_and_type2_error_compare_num.pdf-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/hardness/type1_and_type2_error_compare_num.pdf-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/hardness/type1_and_type2_error_compare_num.pdf-1400.webp"/> <img src="/assets/img/blog/hardness/type1_and_type2_error_compare_num.pdf" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Type I and type II error tradeoff when selecting bandwidth for kernel C. Small training size corresponds to worse conditional mean estimates. </div> <p>This creates a fundamental tradeoff:</p> <ul> <li>A wider kernel smooths aggressively → lower Type I error, but reduced power.</li> <li>A narrower kernel localizes aggressively → lower Type II error, but increased risk of Type I inflation.</li> </ul> <p>Optimizing for power can therefore push us directly into spurious rejection.</p> <p>Conditional independence testing is fragile not only because dependence may be subtle,<br/> but because the very act of searching for it can create it.</p> <h3 id="the-central-lesson">The central lesson</h3> <p>CI testing does not merely require detecting dependence.</p> <p>It requires:</p> <blockquote> <p>Estimating conditional means accurately enough that regression error does not masquerade as conditional dependence.</p> </blockquote> <p>That is an extremely strong requirement.</p> <p>And that is why CI testing fails in practice.</p> <h2 id="final-summary">Final Summary</h2> <h3 id="takeaways-why-ci-is-hard-both-theoretically-and-practically">Takeaways: why CI is hard both theoretically and practically</h3> <p>Conditional independence testing is difficult for structural, not accidental, reasons:</p> <ol> <li> <p><strong>Dependence can hide in subtle structure.</strong><br/> Conditional dependence may be localized, oscillatory, or globally canceling — detectable only at the right scale.</p> </li> <li> <p><strong>The kernel on \(C\) controls what structure is visible.</strong><br/> Over-smoothing misses real dependence (Type II error).<br/> Over-localizing amplifies noise and instability.</p> </li> <li> <p><strong>Regression error induces spurious dependence.</strong><br/> Imperfect estimation of \(\mathbb{E}[\phi_A(A) \mid C]\) and \(\mathbb{E}[\phi_B(B) \mid C]\) introduces artificial residual correlation, even under the null.</p> </li> <li> <p><strong>Kernel selection can overfit regression bias.</strong><br/> Optimizing the kernel on \(C\) for power may align the test with structured regression error rather than true signal.</p> </li> <li> <p><strong>Null approximations rely on ideal asymptotics.</strong><br/> When regression error prevents degeneracy, the statistic is no longer centered at zero, and Type I error can inflate dramatically.</p> </li> </ol> <p>In short:</p> <blockquote> <p>CI testing is hard not just because dependence is difficult to detect,<br/> but because false dependence is easy to create.</p> </blockquote> <h3 id="practical-recommendations">Practical recommendations:</h3> <p>Though the theory is pessimistic, CI testing can still be useful in practice — if we treat it as a fragile procedure and design the pipeline accordingly.</p> <ul> <li> <p><strong>Sample splitting.</strong><br/> Use an independent training set to estimate the conditional means \(\hat{\mu}_{A|C}\) and \(\hat{\mu}_{B|C}\), and a separate test set to compute the KCI statistic and calibrate the null.<br/> As a rule of thumb, the training set should be at least as large as the test set, and preferably much larger (how much larger depends on the complexity of \(\mathbb{E}[A|C]\) and \(\mathbb{E}[B|C]\)).</p> </li> <li> <p><strong>Strong regression (bias matters more than you think).</strong><br/> Use flexible, low-bias regression models for conditional mean estimation. CI testing is extremely sensitive to systematic regression error: even small structured bias can look like conditional dependence under \(H_0\).</p> </li> <li> <p><strong>Kernel choice for power (but only on the training split).</strong><br/> If you tune the conditioning kernel \(k_C\) (e.g., its bandwidth) to improve power, do it using only the training split—e.g., by maximizing an estimated signal-to-noise ratio (SNR). Then <em>freeze</em> the choice and evaluate on the test split.</p> </li> <li> <p><strong>Be cautious about “discoveries.”</strong><br/> Even with all safeguards, it is still easy to trick yourself: power tuning can overfit to regression artifacts, and null calibration can silently fail when regression error does not decay fast enough. Treat borderline rejections with skepticism and stress-test them with sensitivity checks.</p> </li> </ul> <p>Paper： <a href="https://arxiv.org/abs/2512.14000">On the Hardness of Conditional Independence Testing In Practice</a></p> <p>Code： <a href="https://github.com/he-zh/kci-hardness">github.com/he-zh/kci-hardness</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-hardness.bib"></d-bibliography></div> <footer class="sticky-bottom mt-5"> <div class="container"> &copy; Copyright 2026 Zheng HE. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-MW61VMKDBR"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-MW61VMKDBR");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>