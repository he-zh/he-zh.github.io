<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>A note on kernel methods | Zheng HE</title> <meta name="author" content="Zheng HE"/> <meta name="description" content="for readers who want to know or have a quick reference to kernels + RKHS basics."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/H.png?1388fb2ffcaa33939e29e300e7ac9694"/> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://he-zh.github.io/blog/2024/kernel/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Zheng&nbsp;</span>HE</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">A note on kernel methods</h1> <p class="post-meta">November 1, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a> &nbsp; &middot; &nbsp; <a href="/blog/tag/kernel"> <i class="fas fa-hashtag fa-sm"></i> kernel</a> &nbsp; <a href="/blog/tag/note"> <i class="fas fa-hashtag fa-sm"></i> note</a> &nbsp; &nbsp; &middot; &nbsp; <a href="/blog/category/basics"> <i class="fas fa-tag fa-sm"></i> basics</a> &nbsp; </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Kernel methods are one of the most elegant bridges between linear algebra, functional analysis, and modern machine learning. Once you see the pattern‚Äî<em>map into a Hilbert space, do linear operations there</em>‚Äîeverything from SVMs to distribution comparison becomes a special case.</p> <hr/> <h3 id="1-preliminaries-the-geometry-of-functions">1. Preliminaries: The Geometry of Functions</h3> <p>To understand kernels, we first need to define the spaces in which they live. We transition from finite-dimensional vectors to infinite-dimensional function spaces.</p> <h4 id="11-function-spaces">1.1 Function Spaces</h4> <p>A function space \(\mathcal{F}\) is a vector space where the ‚Äúpoints‚Äù are functions.</p> <p><strong>Definition.</strong> Let \(f, f' \in \mathcal{F}\) correspond to weight vectors \(w, w'\). The operations are defined point-wis</p> <ul> <li>Addition/Scaling: \((\alpha f + f')(x) = \alpha f(x) + f'(x)\).</li> <li>Inner Product: \(\langle f, f' \rangle = w \cdot w'\).</li> <li>Norm: \(\|f\|_{\mathcal{F}} = \sqrt{\langle f, f \rangle} = \|w\|\).</li> </ul> <h4 id="12-the-l2x-mu-space">1.2 The \(L^2(X, \mu)\) Space</h4> <p>In probability and measure theory, we often care about functions whose ‚Äúsize‚Äù is finite under an integral.</p> <p><strong>Definition.</strong> \(L^2(X, \mu)\) consists of all measurable functions \(f\) such that:</p> \[\|f\|_2 = \left( \int_X |f(x)|^2 d\mu(x) \right)^{1/2} &lt; \infty\] <h4 id="13-hilbert-space">1.3 Hilbert Space</h4> <p>A Hilbert space is essentially a vector space equipped with an inner product that is complete (meaning all Cauchy sequences converge within the space).</p> <p>Cauchy Sequence: For every \(\epsilon &gt; 0\), there exists \(N\) such that \(\|f_n - f_m\|_{\mathcal{F}} &lt; \epsilon\) for all \(m, n \ge N\).</p> <p>Completeness: Ensures we can perform calculus and optimization without ‚Äúfalling out‚Äù of the space.</p> <hr/> <h3 id="2-kernels-and-the-rkhs">2. Kernels and the RKHS</h3> <p>The ‚ÄúKernel Trick‚Äù allows us to compute inner products in high-dimensional feature spaces without ever explicitly computing the coordinates of the data.</p> <h4 id="21-what-is-a-kernel">2.1 What is a Kernel?</h4> <p><strong>Definition.</strong> A function \(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) is a positive definite kernel if there exists a Hilbert space \(\mathcal{H}\) and a feature map \(\phi(x): \mathcal{X} \to \mathcal{H}\) such that:</p> \[k(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}\] <p>The Mercer Condition: For \(k\) to be a valid kernel, the kernel matrix \(K\) (where \(K_{ij} = k(x_i, x_j)\)) must be Symmetric Positive Semi-Definite (SPSD).</p> <p>This means: \(\forall \alpha \in \mathbb{R}^m, \alpha^\top K \alpha \ge 0\).</p> <p>All eigenvalues of \(K\) are non-negative.</p> <p>In machine learning, commonly used kernels include the Gaussian and Laplace kernels, i.e.,</p> \[k(x,x')=\exp\left(-\frac{\\mid X-x'\|^2_2}{2\sigma^2}\right), \qquad k(x,x')=\exp\left(-\frac{\\mid X-x'\|_1}{\sigma}\right)\] <p>where \(\sigma &gt; 0\) is a bandwidth parameter. These kernels belong to a class of kernel functions called radial basis functions (RBF).</p> <p><strong>Universal and characteristic kernel</strong> <a href="https://jmlr.org/papers/volume12/sriperumbudur11a/sriperumbudur11a.pdf">(reference)</a></p> <p>Universal and characteristic‚Äîhave been developing in parallel in machine learning: universal kernels are proposed in the context of achieving the Bayes risk by kernel-based classification/regression algorithms while characteristic kernels are introduced in the context of distinguishing probability measures by embedding them into a reproducing kernel Hilbert space (RKHS).</p> <h4 id="22-reproducing-kernel-hilbert-space-rkhs">2.2 Reproducing Kernel Hilbert Space (RKHS)</h4> <p>The RKHS is a special Hilbert space where evaluation is a bounded linear functional.</p> <p><strong>Evaluation Functional</strong>:</p> <ul> <li>For each \(x\in X\), the evaluation functional \(Lx\)‚Äã is a map that takes a function \(f‚ààH\) and returns its value at ùë•: \(Lx‚Äã(f)=f(x)\)</li> <li>This functional ùêøùë•‚Äã is linear, meaning: \(Lx‚Äã(f+g)=Lx‚Äã(f)+Lx‚Äã(g)\) and \(Lx‚Äã(Œ±f)=Œ±Lx‚Äã(f)\)</li> </ul> <p><strong>Definition (Reproducing kernel Hilbert space RKHS).</strong> A Hilbert space \(\mathcal{H}\) of functions is a reproducing kernel Hilbert space (RKHS) if the evaluation functionals \(F_x[f]\) defined as \(F_x[f] = f(x)\) are bounded, i.e., for all \(x ‚àà X\) there exists some \(C &gt; 0\) such that</p> \[\|F_x[f]\|=f(x)\le C\|f\|_{\mathcal{H}}, \qquad \forall f \in \mathcal{H}\] <p>Interpretation: In many spaces, two functions can be ‚Äúclose‚Äù in norm but have very different values at a specific point \($X\)$. In an RKHS, if functions are close in norm, their values \(f(x)\) are also close.</p> <p><strong>The Reproducing Property:</strong> For every \(x \in X\), there exists a function \(k_x \in \mathcal{H}\) (the representer of evaluation) such that:</p> \[f(x) = \langle f, k_x \rangle_{\mathcal{H}}\] <p>If we set \(f = k_y\) (or written as \(k(\cdot, y)\)), we get the kernel itself:</p> \[k(x, y) = \langle k_x, k_y \rangle_{\mathcal{H}}\] <p>The RKHS \(\mathcal{H}\) is fully characterized by the reproducing kernel \(k\).</p> <p><strong>Theorem.</strong> For every positive definite function \(k(¬∑, ¬∑)\) on \(X √ó X\) there exists a unique (up to isometric isomorphism) RKHS with \(k\) as its reproducing kernel. Conversely, the reproducing kernel of an RKHS is unique and positive definite.</p> <p><strong>The kernel k generates the RKHS, i.e. \(\mathcal{H}=\text{span}\{k(x,\cdot)\mid x\in X\}\)</strong></p> <p>define \(\phi(x) := k(x,\cdot)=\{x' \to k(x,x')\}\) for all \($X\)$. \(\mathcal{H}\) is the set of all linear combinations of these functions \(\sum_{i=1}^m \alpha_i \phi(x_i)\) for all \(x_1, \dots, x_m \in \mathcal{X}\), \(\alpha_1, \dots, \alpha_m \in \mathbb{R}\).</p> <p>Also, we have</p> \[\left\langle \sum_{i=1}^m \alpha_i \phi(x_i) , \sum_{j=1}^m \beta_j \phi(x'_j)\right\rangle_\mathcal{H} = \sum_{i=1}^m \sum_{j=1}^m \alpha_i \beta_j K(x_i, x_j')\] <p>By virtue of reproducing property, we know</p> <ul> <li>\(k(x, ¬∑)\) is a high-dimensional representer of \($X\)$</li> <li>by the reproducing property it also acts as a representer of evaluation of any function in \(\mathcal{H}\) on the data point \($X\)$</li> </ul> <p><strong>Definition (Separable Hilbert Space).</strong> A Hilbert space \(\mathcal{H}\) is said to be separable if it has a countable basis.</p> <p>If \(a\in \mathcal{H}\) and \((e_i)_{i\in I}\) is an orthonormal basis for \(\mathcal{H}\). The index sets \(I\) are assumed to be either finite or countably infinite. Then</p> \[a = \sum_{i‚ààI} \langle a, e_i\rangle_{\mathcal{H}} e_i .\] <p>Most we dealt with is separable. example of non-separable \(k(x,y)=\mathbb{(x=y)}\)</p> <h5 id="221-tensor-product-reference">2.2.1 Tensor Product <a href="https://www.uio.no/studier/emner/matnat/math/nedlagte-emner/MAT-INF2360/v12/tensortheory.pdf">(reference)</a></h5> <p><strong>Definition(Tensor product of vectors).</strong><br/> If x, y are vectors of length M and N, respectively, their tensor product \(x‚äóy\) is defined as the \(M √óN\)-matrix defined by \((x ‚äó y)_{ij} = x_iy_j\) . In other words, \(x ‚äó y = xy^T\).</p> <p><strong>Definition 7.3 (Tensor product of matrices).</strong> If \(S : R^M ‚Üí R^M\) and \(T : R^N ‚Üí R^N\) are matrices, we define the linear mapping \(S ‚äó T : L_{M,N}(R) ‚Üí L_{M,N}(R)\) by linear extension of \((S ‚äó T)(e_i ‚äó e_j )=(Se_i) ‚äó (Te_j )\). The linear mapping \(S ‚äó T\) is called the tensor product of the matrices S and T.</p> <p><strong>Tensor product</strong></p> <ul> <li>\(\langle f, k_X \rangle\) \(\langle g, l_Y\rangle = \langle f, k_X‚äól_Y g \rangle\)</li> <li>\((a‚äób) x = \langle x, b\rangle a\), \((ba^T)f=(a^Tf)b\)</li> <li>\(\langle g\otimes f , \phi(Y) \otimes \psi(X) \rangle_{\mathcal{G} \otimes \mathcal{H}}\)=\(\langle g, \psi(Y)\rangle_{\mathcal{G}} \langle \psi(X),f\rangle_{\mathcal{H}}\)</li> <li>\((S\otimes T)(x\otimes y)\) = \((Sx)\otimes(Ty)\)</li> <li>\((S_1 ‚äó T_1)(S_2 ‚äó T_2)\) = \((S1S2) ‚äó (T_1T_2)\)</li> <li>\((S ‚äó T)X = SXT^T\) if S, T are matrices</li> </ul> <h5 id="222-hilbert-schmidt-operator-reference">2.2.2 Hilbert-Schmidt Operator <a href="https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture5_covarianceOperator.pdf">reference</a></h5> <p><strong>Hilbert-Schmidt operator.</strong> Let \(\mathcal{F}\) and \(\mathcal{G}\) be separable Hilbert spaces and \((f_i)_{i‚ààI}\) and \((g_j)_{j‚ààJ}\) are orthonormal basis for \(\mathcal{F}\) and \(\mathcal{G}\), respectively. A Hilbert-Schmidt operator is a <em>bounded</em> operator \(\mathcal{A}: \mathcal{G}\to\mathcal{F}\) whose Hilbert-Schmidt norm</p> \[\|\mathcal{A}\|_{HS}^2 = \sum_{j\in J} \|\mathcal{A}g_j\|_{\mathcal{F}}^2=\sum_{i\in I}\sum_{j\in J} |\langle \mathcal{A} g_j, f_i\rangle_\mathcal{F}|^2\] <p>is <em>finite</em> (\(\|\mathcal{A}\|_{HS}^2 &lt; \infty\)).</p> <p>The Hilbert-Schmidt operators mapping from \(\mathcal{G}\) to \(\mathcal{F}\) form a Hilbert space, written \(HS(\mathcal{G},\mathcal{F})\), with inner product</p> \[\langle L, M\rangle_{HS} = \sum_{j\in J} \langle L g_j, M g_j\rangle_\mathcal{F}=\sum_{i\in I}\sum_{j \in J} \langle L g_i, f_j\rangle_\mathcal{F}\langle L g_i, M f_j\rangle_\mathcal{F}\] <p>Given \(b ‚àà \mathcal{G}\) and \(a ‚àà \mathcal{F}\), we define the tensor product \(a‚äób\) as a rank-one operator from \(\mathcal{G}\) to \(\mathcal{F}\),</p> \[(a\otimes b)g \mapsto \langle g,b \rangle_\mathcal{G} a\] <p>The Hilbert-Schmidt norm is</p> \[\begin{align*} \|a \otimes b\|_{HS}^2 &amp;= \sum_{j\in J} \|(a \otimes b) g_j \|_{\mathcal{F}}^2\\ &amp;= \sum_{j\in J} \|\langle g_j ,b \rangle_\mathcal{G} a \|_{\mathcal{F}}^2\\ &amp;= \sum_{j\in J} |\langle g_j ,b \rangle_\mathcal{G}|^2 \| a \|_\mathcal{F}^2\\ &amp;= \|b\|_\mathcal{G}^2 \|a\|_{\mathcal{F}}^2 \end{align*}\] <p><strong>Definition (bounded operator).</strong><br/> A linear operator \(A : F ‚Üí R\) is bounded when</p> \[Af \le \lambda_A \|f\|_\mathcal{F} \quad \forall f \in \mathcal{F}\] <p><strong>Property.</strong></p> <ul> <li>\(\langle L, a\otimes b \rangle_{HS}=\langle a, Lb \rangle_\mathcal{F}\), given \(a\otimes b, L ‚àà HS(\mathcal{G}, \mathcal{F})\),</li> <li>\(\langle u\otimes v, a\otimes b \rangle_{HS}=\langle u, a \rangle_\mathcal{F} \langle b, v \rangle_\mathcal{G}\),</li> <li>\(\|\phi(x) \otimes \psi(y)\|_{HS} = \|\phi(x)\|_{\mathcal{F}} \|\psi(y)\|_{\mathcal{G}}=\sqrt{k(x,x) l(y,y)}\),</li> <li>\(\langle C_{XY}, f\otimes g\rangle_{HS} = E_{xy}\langle \phi(x)\otimes \psi(y), f\otimes g\rangle_{HS} = E_{xy} [\langle f, \phi(x)\rangle_{\mathcal{F}}, \langle g, \psi(y)\rangle_{\mathcal{G}}]=E_{xy}[f(x)g(y)]\),</li> <li>\(\langle C_{XY}, C_{XY}\rangle_{HS} = E_{x,y}E_{x',y'}k(x,x')l(y,y')\),</li> <li>\(\langle \mu_{X} \otimes \mu_{Y}, \mu_{X} \otimes \mu_{Y} \rangle_{HS} = E_{x,x'}E_{y,y'}k(x,x')l(y,y')\),</li> <li>In vector space, \(\langle C_{XY}, A\rangle_{HS} = trace(C_{XY}^\top A)=\sum_j (C_{XY}g_j)^\top (Ag_j)\).</li> <li>\(\|a \otimes b\|_{HS} = \|a\|_{\mathcal{F}}^2 \|b\|_{\mathcal{G}}^2\).</li> </ul> <hr/> <h3 id="3-kernel-mean-embeddings">3. Kernel Mean Embeddings</h3> <p>Kernel mean embeddings allow us to represent entire probability distributions as a single point (a function) in an RKHS.</p> <p><strong>Definition.</strong> The mean embedding of a distribution \(\mathbb{P}\) is:</p> \[\mu_{\mathbb{P}} := \mathbb{E}_{X \sim \mathbb{P}} [k(X, \cdot)] = \int_{\mathcal{X}} \phi(x) d\mathbb{P}(x)\] <p>Why is this useful?</p> <ul> <li>If the kernel is ‚Äúcharacteristic‚Äù (like Gaussian or Laplace), the mapping \(\mathbb{P} \to \mu_{\mathbb{P}}\) is injective. This means \(\mu_{\mathbb{P}}\) contains all information about the distribution. This means that \(\|{\mu_\mathbb{P}-\mu_\mathbb{Q}}\|\) if and only if \(\mathbb{P}=\mathbb{Q}\).</li> <li>If \(\mathbb{P}\) and \(\mathbb{Q}\) are close in probability distance measures, then \(\mu_\mathbb{P}\) is also close to \(\mu_\mathbb{Q}\) in the \(\|\cdot \|_{\mathcal{H}}\) norm</li> </ul> <p>**Expectations as Inner Products: ** To calculate the expected value of a function \(f\), we simply take an inner product:</p> \[\mathbb{E}_{\mathbb{P}}[f(x)] = \langle f, \mu_{\mathbb{P}} \rangle_{\mathcal{H}}\] <p><strong>Lemma (<a href="https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture5_covarianceOperator.pdf">Existence of mean embedding</a>).</strong> If \(\mathbb{E}_{X‚àº\mathbb{P}}[\sqrt{k(X, X)}] &lt; \infty\), then \(\mu_\mathbb{P}\in\mathcal{H}\) and \(\mathbb{E}_{X‚àº\mathbb{P}}[f(X)]=‚ü®f,\mu_{\mathbb{P}}‚ü©_\mathcal{H}\).</p> <hr/> <h3 id="4-operators-covariance-and-conditional-mean">4. Operators: Covariance and Conditional Mean</h3> <p>Just as we have covariance matrices for vectors, we have operators for functions.</p> <h4 id="41-cross-covariance-operator">4.1 Cross-Covariance Operator</h4> <p><strong>Definition.</strong> The uncentered cross-covariance operator \(C_{YX}: \mathcal{H} \to \mathcal{G}\) is:</p> \[C_{YX} := \mathbb{E}_{YX} [\phi(Y) \otimes \phi(X)]\] <p>It satisfies the property:</p> <p>\(\langle g, C_{YX} f \rangle_{\mathcal{G}} = \text{Cov}(g(Y), f(X))\).</p> <p>Centered cross-covariance operator is</p> \[\tilde C_{YX} :=\mathbb{E}_{YX}[œÜ(Y)‚äó\phi(X)] - \mu_{P_Y} ‚äó \mu_{P_X}=Œº_{P_{YX}} - \mu_{P_Y ‚äó P_X},\] <p>Equivalently, we can define an operator \(C_{YX}\) as a unique bounded operator that satisfies</p> \[\langle g, C_{YX}f\rangle = Cov[g(Y),f(X)]\] <p>for all \(f \in \mathcal{H}\) and \(g \in \mathcal{G}\).</p> <p><strong>Covariance operator.</strong> If \(X = Y\) , we call \(C_{XX}\) the covariance operator.</p> <ul> <li>\((C_{YX}f)(\cdot)=\int_{X\times Y}l(\cdot,y)f(x) dP_{XY}(x,y)\).</li> <li>\((C_{XX}f)(\cdot)=\int_{X}k(\cdot,x)f(x) dP_{X}(x)\).</li> </ul> <p><strong>Theorem.</strong> If \(\mathbb{E}_{YX}[g(Y)\mid X=¬∑]‚àà \mathcal{H}\) for \(g‚àà\mathcal{G}\), then</p> \[C_{XX}\mathbb{E}_{Y\mid X}[g(Y)\mid X =¬∑]=C_{XY} g\] <details><summary>Proof for the theorem</summary> \[\begin{align*} C_{XX}\mathbb{E}_{Y\mid X}[g(Y)\mid X =¬∑] &amp;= \mathbb{E}_{XX}[\phi(X)‚äó\phi(X)] \mathbb{E}_{Y\mid X}[g(Y)\mid X =¬∑]\\ &amp;= \int_{X}\int_Y \phi(x)\otimes\phi(x) \langle g , \varphi(y)\rangle_{\mathcal{G}} dP_X(x)dP_{Y\mid X}(y\mid X) \\ &amp;= \int_X\int_Y \phi(x)\otimes\phi(x) \langle g , \varphi(y)\rangle_{\mathcal{G}} dP_{XY}(x,y) \\ \end{align*}\] </details> <p><strong>Empirical estimate of the centered \(\tilde C_{YX}\):</strong></p> \[\begin{align*} \hat C_{YX}&amp;= \frac{1}{n} \sum_{i=1}^{n} l(y_i,\cdot)\otimes k(x_i,\cdot) - \hat \mu_{P_Y} \otimes \hat \mu_{P_X} \\ &amp;=\frac{1}{n} \sum_{i=1}^{n} \{l(y_i,\cdot)-\hat \mu_{P_Y} \} \otimes \{k(x_i,\cdot)-\hat\mu_{P_X} \} \\ &amp;= \frac1n (\Psi - \hat\mu_{P_Y} \mathbf{1}^\top)(\Phi - \hat\mu_{P_Y} \mathbf{1}^\top)^\top \\ &amp;= \frac1n (\Psi (I-\frac1n \mathbf{1} \mathbf{1}^\top))(\Phi (I-\frac1n \mathbf{1} \mathbf{1}^\top))^\top \\ &amp;= \frac1n \Psi (I-\frac1n \mathbf{1} \mathbf{1}^\top) \Phi^\top \\ &amp;=\frac1n \Psi H \Phi^\top \\ \end{align*}\] <p>where \(H = I_n ‚àí \frac1n \mathbb{1}_n\) is the centering matrix with \(\mathbb{1}_n\) an n √ó n matrix of ones, \(\Psi = (œÜ(y_1),...,œÜ(y_n))\) and \(\Phi = (\phi(x_1),...,\phi(x_n))\)</p> <h4 id="42-conditional-mean-embedding-cme">4.2 Conditional Mean Embedding (CME)</h4> <p>The CME represents the conditional distribution \(P(Y\mid X=x)\) as an element in \(\mathcal{G}\).</p> <p>Suppose \(X\) and \(Y\) be measurable spaces and let \(X\) and \(Y\) be random variables taking values in \(X\) and \(Y\) respectively. Assume \(k : X √óX ‚Üí R\) and \(l : Y √ó Y ‚Üí R\) to be positive definite kernels with corresponding RKHS‚Äôs \(H\) and \(G\). Let \(U_{Y\mid X} :H ‚ÜíG\) and \(U_{Y\mid X} ‚ààG\) be conditional mean embeddings of the conditional distribution \(P(Y \mid X)\) and \(P(Y \mid X =x)\)</p> \[U_{Y\mid X} = E_{Y\mid X}[\varphi(Y)\mid X=x] = U_{Y\mid X}k(x,\cdot)\] \[E_{Y\mid X}[g(Y)\mid X=x] = \langle g, U_{Y\mid X} \rangle_\mathcal{G} , \forall g \in \mathcal{G}\] <p><strong>Definition.</strong> Let \(C_{XX} : H ‚Üí H\) and \(C_{YX} : H ‚Üí G\) be the covariance operator of X and cross- covariance operator between X to Y, respectively. Then, the conditional mean embedding \(U_{Y\mid X} and U_{Y\mid X}\) are defined as</p> \[U_{Y\mid X}:= C_{YX} C^{-1}_{XX}\] \[U_{Y\mid X}:=C_{YX}C^{-1}_{XX}k(x,\cdot)\] <p><strong>property</strong></p> <ul> <li>\(E_{Y\mid X}[g(Y)\mid X=x]=\langle g, U_{Y\mid X}\rangle=\langle E_{Y\mid X}[g(Y)\mid X], k(x,\cdot)\rangle\).</li> </ul> <p><strong>Empirical estimate:</strong></p> \[\begin{align*} \hat U_{Y\mid X} &amp;= C_{YX} (C_{XX} + \lambda I)^{-1}\\ &amp;= \Psi (K+\lambda I)^{-1} \Phi^\top\\ \end{align*}\] <p>where \(\Psi:= (\psi(y_1), \dots, \psi(y_n))\), \(\Phi:= (\phi(x_1), \dots, \phi(x_n))\), \(K=\Phi^\top \Phi\) is the Gram matrix for samples from variable X, and \(Œª\) is the regularization parameter to avoid overfitting.</p> \[\begin{align*} \hat U_{Y\mid X} &amp;= \Psi (K+\lambda I)^{-1} \Phi^\top k(x,\cdot)\\ &amp;= \Psi (K+\lambda I)^{-1} K_{:,x} \\ \end{align*}\] <p>where \(K_{:,x} = (k(x,X_1), \dots, k(x,X_n))^\top\)</p> <p>Regression Interpretation: The operator \(U_{Y\mid X} = C_{YX} C_{XX}^{-1}\) is actually the solution to a Vector-Valued Ridge Regression:</p> \[\hat{U}_{Y\mid X} = \arg\min_{U} \sum_{i=1}^n \|\phi(y_i) - U\phi(x_i)\|^2_{\mathcal{G}} + \lambda \|U\|^2_{HS}\] <hr/> <h3 id="references">References</h3> <ul> <li>Sriperumbudur et al. (2011), <em>On the Universality and Characteristic Kernels</em></li> <li>Gretton et al., <em>Notes on Mean Embeddings and Covariance Operators</em></li> <li>Muandet et al. (2017), <em>Kernel Mean Embedding of Distributions: A Review and Beyond</em></li> <li>Song et al. (2013), <a href="https://www.gatsby.ucl.ac.uk/~gretton/papers/SonFukGre13.pdf"><em>Kernel Embeddings of Conditional Distributions</em></a></li> <li>Song et al. (2009), <a href="https://www.ri.cmu.edu/pub_files/2009/6/icml09.pdf"><em>Hilbert Space Embeddings of Conditional Distributions with Applications to Dynamical Systems</em></a></li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/learning-bounds-note/">A note on learning bounds</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/sparsedd/">Sparse double descent where network pruning aggravates overfitting</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/biased-label/">Learning with biased labels</a> </li> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> &copy; Copyright 2026 Zheng HE. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-MW61VMKDBR"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-MW61VMKDBR");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>