@inproceedings{he2022sparse,
  title={Sparse double descent: Where network pruning aggravates overfitting},
  author={He, Zheng and Xie, Zeke and Zhu, Quanzhi and Qin, Zengchang},
  booktitle={International Conference on Machine Learning, ICML},
  pages={8635--8659},
  year={2022},
  organization={PMLR},
  abstract={People usually believe that network pruning not only reduces the computational cost of deep networks, but also prevents overfitting by decreasing model capacity. However, our work surprisingly discovers that network pruning sometimes even aggravates overfitting. We report an unexpected sparse double descent phenomenon that, as we increase model sparsity via network pruning, test performance first gets worse (due to overfitting), then gets better (due to relieved overfitting), and gets worse at last (due to forgetting useful information). While recent studies focused on the deep double descent with respect to model overparameterization, they failed to recognize that sparsity may also cause double descent. In this paper, we have three main contributions. First, we report the novel sparse double descent phenomenon through extensive experiments. Second, for this phenomenon, we propose a novel learning distance interpretation that the curve of l2 learning distance of sparse models (from initialized parameters to final parameters) may correlate with the sparse double descent curve well and reflect generalization better than minima flatness. Third, in the context of sparse double descent, a winning ticket in the lottery ticket hypothesis surprisingly may not always win.},
  arxiv={2206.08684},
  poster={sparsedd_icml2022_poster.pdf},
  code={https://github.com/he-zh/sparse-double-descent},
  selected={true}
}

@article{he2021can,
  title={Can network pruning benefit deep learning under label noise?},
  author={He, Zheng and Zhu, Quanzhi and Qin, Zengchang},
  year={2021}
}

@inproceedings{zhou2021random,
  title={Random Neural Graph Generation with Structure Evolution},
  author={Zhou, Yuguang and He, Zheng and Wan, Tao and Qin, Zengchang},
  booktitle={Neural Information Processing: 28th International Conference, ICONIP},
  pages={87--98},
  year={2021},
  organization={Springer International Publishing},
  abstract={In deep learning research, typical neural network models are multi-layered architectures, and weights are tuned while optimizing a carefully designed loss function. In recent years, studies of randomized neural networks have been extended towards deep architectures, opening a new research direction to the design of deep learning models. However, how the structure of the network can influence the model performance still remains unclear. In this paper, we move a further step to investigate the relation between network topology and performance via a structure evolution algorithm. Experimental results show that the graph would evolve towards a more small-world topology at the beginning of the training session along with gaining accuracy, and would also evolve towards a structure with more scale-free property in the following periods. These conclusions could help explain the effectiveness of the randomly connected networks, as well as give us insights in new possibilities of network architecture design.},
  html={https://link.springer.com/chapter/10.1007/978-3-030-92270-2_8},
  selected={true}
}
