<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://he-zh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://he-zh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-19T05:00:36+00:00</updated><id>https://he-zh.github.io/feed.xml</id><title type="html">blank</title><subtitle>Academic website of Zheng HE. </subtitle><entry><title type="html">Why testing conditional independence is so hard?</title><link href="https://he-zh.github.io/blog/2025/ci-hardness/" rel="alternate" type="text/html" title="Why testing conditional independence is so hard?"/><published>2025-12-10T00:00:00+00:00</published><updated>2025-12-10T00:00:00+00:00</updated><id>https://he-zh.github.io/blog/2025/ci-hardness</id><content type="html" xml:base="https://he-zh.github.io/blog/2025/ci-hardness/"><![CDATA[<p>Conditional independence (CI) testing is widely used in causal discovery, scientific modeling, fairness, domain generalization, and robustness analysis. And yet it often fails in practice.</p> <p>Why?</p> <p>Let‚Äôs unpack the story.</p> <h2 id="background">Background</h2> <p>Before we talk about hardness, we need to understand what conditional independence really means ‚Äî and how we try to measure it.</p> <h3 id="conditional-independence">Conditional independence</h3> <p>What does conditional independence between \(A\) and \(B\) given \(C\) mean?</p> <p>At a high level:</p> <blockquote> <p>Once we account for \(C\), does \(B\) still tell us anything about \(A\)?</p> </blockquote> <p>Two concrete examples help build intuition.</p> <h3 id="example-1-fairness-in-lending">Example 1: fairness in lending</h3> <ul> <li>\(A\) = loan decision</li> <li>\(B\) = race</li> <li>\(C\) = credit score, income</li> </ul> <p>Race and loan decisions may be correlated marginally.<br/> But once we control for creditworthiness, race <em>should</em> no longer influence the decision.</p> <p>If it still does, the system may be unfair.</p> <h3 id="example-2-distribution-shift">Example 2: distribution shift</h3> <ul> <li>\(A\) = model prediction</li> <li>\(B\) = time of day</li> <li>\(C\) = true location</li> </ul> <p>A model might use ‚Äútime of day‚Äù as a shortcut for predicting location.<br/> If so, its predictions will fail when time distributions shift.</p> <p>If predictions remain independent of time once we condition on true location, the model is robust.</p> <h3 id="formal-definition">Formal definition</h3> <p>Conditional independence is defined as</p> \[A \perp\!\!\!\perp B \mid C \quad \Longleftrightarrow \quad P_{A,B \mid C} = P_{A \mid C} P_{B \mid C}.\] <blockquote> <p>Once \(C\) is fixed, knowing \(B\) provides no additional information about \(A\).</p> </blockquote> <p>Equivalently, for almost every value of \(C\),</p> \[\text{Cov}(f(A), g(B) \mid C) = 0\] <p>for all measurable functions \(f\) and \(g\).</p> <p>This functional view turns out to be crucial.</p> <h3 id="ci-as-a-hypothesis-testing-problem">CI as a hypothesis testing problem</h3> <p>At its core, hypothesis testing asks:</p> <ul> <li><strong>Null hypothesis (\(H_0\)):</strong> nothing interesting is happening.</li> <li><strong>Alternative hypothesis (\(H_1\)):</strong> something is going on.</li> </ul> <p>In CI testing:</p> \[H_0: A \perp\!\!\!\perp B \mid C\] \[H_1: A \not\!\perp\!\!\!\perp B \mid C\] <p>We compute a statistic from data. If it is too extreme, we reject \(H_0\).</p> <p>Two types of error can occur:</p> <ul> <li> <p><strong>Type I error (false positive):</strong><br/> Rejecting \(H_0\) when conditional independence actually holds.<br/> In CI testing, this means concluding that \(A\) and \(B\) are conditionally dependent when they are not.</p> </li> <li> <p><strong>Type II error (false negative):</strong><br/> Failing to reject \(H_0\) when conditional dependence actually exists.<br/> In CI testing, this means missing real conditional structure.</p> </li> </ul> <p>A good test aims to:</p> <ul> <li>Control Type I error at a predefined level \(\alpha\).</li> <li>Minimize Type II error (maximize power).</li> </ul> <p>For many classical testing problems, this tradeoff is manageable.</p> <p>For conditional independence, both errors are unusually difficult to control simultaneously.</p> <h3 id="measuring-conditional-independence">Measuring conditional independence</h3> <p>A characterization of conditional independence is: \(A \perp\!\!\!\perp B \mid C\) if and only if, for all square-integrable functions \(f \in L_A^2\), \(g \in L_B^2\), \(w \in L_C^2\),</p> \[\mathbb{E}_{C}\Big[ w(C) \, \mathbb{E}_{AB|C}\Big[ \big(f(A)- \mathbb{E}[f(A)|C]\big) \big(g(B)- \mathbb{E}[g(B)|C]\big) \Big] \Big] = 0.\] <p>Let‚Äôs unpack this.</p> <ul> <li> <p>First, we measure conditional covariance by \(f(A) - \mathbb{E}[f(A)|C]\) and \(g(B) - \mathbb{E}[g(B)|C]\)</p> </li> <li> <p>Then we weight conditional covariance by \(w(C)\) to emphasize specific regions of \(C\).</p> </li> </ul> <p>If any conditional covariance remains on a region of \(C\) with non-negligible probability, an appropriate \(w(C)\) will detect it.</p> <p>So conditional independence means:</p> <blockquote> <p>No residual dependence remains after removing the effect of \(C\).</p> </blockquote> <h3 id="rkhs-view-from-functions-to-operators">RKHS view: from functions to operators</h3> <p>It is impossible to test all square-integrable functions, thus we use functions in a Reproducing Kernel Hilbert Space (RKHS).</p> <p>An RKHS \(\mathcal{H}_A\) contains functions that are linear w.r.t. \(\phi_A(a)\)</p> \[f(a) = \langle w, \phi_A(a) \rangle,\] <p>where \(\phi_A\) is a feature map.</p> <p>Define the conditional mean embedding</p> \[\mu_{A|C}(c) = \mathbb{E}[ \phi_A(A) \mid C = c ].\] <p>It satisfies</p> \[\langle \mu_{A|C}(c), f \rangle_{\mathcal{H}_A} = \mathbb{E}[ f(A) \mid C = c ].\] <p>So conditional expectations become inner products in Hilbert space.</p> <p>We define the conditional cross-covariance operator</p> \[\mathcal{C}_{AB|C}(c) = \mathbb{E}_{AB|C}\Big[ \big(\phi_A(A)-\mu_{A|C}(c)\big) \otimes \big(\phi_B(B)-\mu_{B|C}(c)\big) \mid C=c \Big].\] <p>This operator satisfies</p> \[\langle f \otimes g, \mathcal{C}_{AB|C}(c) \rangle = \text{Cov}(f(A), g(B) \mid C=c).\] <p>So it encodes <em>all</em> conditional covariances.</p> <h3 id="the-kci-operator">The KCI operator</h3> <p>To aggregate over \(C\), we define</p> \[\mathcal{C}_{\text{KCI}} = \mathbb{E}_C\Big[ \mathcal{C}_{AB|C}(C) \otimes \phi_C(C) \Big].\] <p>For any test functions \(f, g, w\),</p> \[\langle f \otimes g, \mathcal{C}_{\text{KCI}}\, w \rangle = \mathbb{E}_C\Big[ w(C) \, \mathbb{E}_{AB|C} \Big[ (f(A)-\mathbb{E}[f(A)|C]) (g(B)-\mathbb{E}[g(B)|C]) \Big] \Big].\] <p>If the RKHSs \(\mathcal{H}_A, \mathcal{H}_B, \mathcal{H}_C\) are sufficiently rich (e.g., \(L^2\)-universal), then</p> \[\mathcal{C}_{\text{KCI}} = 0 \quad \Longleftrightarrow \quad A \perp\!\!\!\perp B \mid C.\] <p>A common test statistic <d-cite key="zhang2011kernel"></d-cite> is</p> \[\text{KCI} = \|\mathcal{C}_{\text{KCI}}\|_{\text{HS}}^2.\] <p>So the problem reduces to:</p> <blockquote> <p>Estimate this operator from finite samples and determine whether it is zero.</p> </blockquote> <p>That is where the real difficulty begins.</p> <h2 id="why-ci-testing-is-fundamentally-hard">Why CI testing is fundamentally hard</h2> <h3 id="the-binary-embedding-trick">The binary embedding trick</h3> <p>Now comes the key construction.</p> <p>Start with any distribution of scalars \(A, B, C\) such that</p> \[A \not\perp B \mid C.\] <p>So conditional dependence genuinely exists.</p> <p>Now perform the following transformation.</p> <ol> <li>Sample scalars \(A, B, C\) where \(A \not\!\perp\!\!\!\perp B \mid C\)</li> <li>Take binary expansions of \(A, B, C\).</li> <li>Truncate each to 100 bits: \(A_{100}, \quad B_{100}, \quad C_{100}.\)</li> <li>Embed \(A_{100}\) into \(C_{100}\) by concatenation. For example: <ul> <li> \[C_{100} = 10011001\dots\] </li> <li>\(A_{100} = 10111100\dots\)<br/> The the new \(C\) is:</li> </ul> </li> </ol> \[C_\text{new} = (C_{100} \text{ bits} || A_{100} \text{ bits}) = 10011001...10111100...\] <p>So the new conditioning variable contains:</p> <ul> <li>the original first 100 bits of \(C\), and</li> <li>the first 100 bits of \(A\).</li> </ul> <p>Finally, add an arbitrarily small continuous noise to all binary variables so the joint distribution remains absolutely continuous.</p> <h3 id="what-just-happened">What just happened?</h3> <p>After this construction:</p> <ul> <li>\(A_{100}\) can be reconstructed from \(C_{100}\)</li> <li>Therefore, once we condition on \(C_{\text{new}}\), \(B_{\text{new}}\) contains no additional information beyond what is already encoded in \(A_{\text{new}}\).</li> </ul> <p>As a result,</p> \[A_\text{new} \perp\!\!\!\perp B_\text{new} \mid C_\text{new}.\] <p>The conditional dependence has disappeared.</p> <p><strong>The insight here is:</strong></p> <p>Nothing dramatic happened to the distribution at a coarse scale. The only change was that extremely fine-grained information about \(A\) was embedded in the tail digits of \(C\).</p> <p>To detect this transformation, a test would need effectively infinite precision ‚Äî it would have to examine arbitrarily fine features of the joint distribution.</p> <p>No finite-sample test can reliably do this.</p> <p>That is the essence of the impossibility result:</p> <blockquote> <p>Evidence for conditional independence can be hidden in arbitrarily subtle features of the distribution.</p> </blockquote> <p>And no finite dataset can rule out such constructions.</p> <h3 id="the-shahpeters-impossibility-theorem">The Shah‚ÄìPeters impossibility theorem</h3> <p>This construction is not just a clever trick. It reflects a deep structural limitation formalized by Shah and Peters (2020)<d-cite key="shah2018hardness"></d-cite>.</p> <blockquote> <p>For any finite-sample conditional independence test, and for any alternative distribution where \(A \not\!\perp\!\!\!\perp B \mid C\), there exists a null distribution (where \(A \perp\!\!\!\perp B \mid C\)) that the test cannot reliably distinguish from that alternative.</p> </blockquote> <p>More concretely:</p> <ul> <li>If your test has power strictly greater than \(\alpha\) against some alternative,</li> <li>Then there must exist at least one null distribution under which the test‚Äôs Type I error exceeds \(\alpha\).</li> </ul> <p>In other words, no CI test can be uniformly valid over all continuous distributions.</p> <p>There is no procedure that simultaneously:</p> <ul> <li>Controls Type I error at level \(\alpha\) for all nulls, and</li> <li>Achieves nontrivial power against all alternatives.</li> </ul> <p>This is not a shortcoming of current algorithms. It is a fundamental limitation of the problem itself.</p> <h2 id="why-it-still-fails-in-practice">Why it still fails in practice</h2> <p>You might think: these are adversarial constructions, surely in practice we don‚Äôt encounter them.</p> <p>Correct ‚Äî we rarely face carefully engineered binary embedding tricks.</p> <p>But the <em>mechanism</em> behind the impossibility result is not artificial.</p> <p>The core issue is this: Conditional dependence can live in structured, localized, or oscillatory features of the distribution. And detecting those features from finite samples is fundamentally delicate.</p> <p>To see how this arises in a realistic setting, consider a problem inspired by engineering diagnostics:</p> <p>Suppose we have high-dimensional vibration data \(C\) collected from a mechanical system.<br/> We want to know whether the behavior of component \(A\) is connected to that of component \(B\), <em>after conditioning on the vibration signal</em>.</p> <p>In many systems:</p> <ul> <li>The overall behavior of each component (predicting \(A\) or \(B\) from \(C\)) depends on broad, low-frequency trends in the vibration data.</li> <li>But any <em>direct coupling</em> between the two components may occur only through narrow, high-frequency resonances.</li> </ul> <p>Detecting these two phenomena requires very different scales of analysis.</p> <p>A kernel that captures broad trends may completely miss high-frequency coupling.<br/> A kernel that zooms in to detect oscillatory resonances may become unstable or amplify noise.</p> <h3 id="a-concrete-example">A Concrete Example</h3> <p>To formalize this, consider the model</p> \[A = f_A(C) + r_A\] \[B = f_B(C) + r_B\] <p>where</p> \[(r_A, r_B) \mid C \sim \mathcal{N} \left( 0, \begin{pmatrix} 1 &amp; \gamma(C) \\ \gamma(C) &amp; 1 \end{pmatrix} \right).\] <p>Here:</p> <ul> <li>\(f_A\) and \(f_B\) capture the systematic effect of \(C\).</li> <li>\(r_A\) and \(r_B\) represent unexplained noise.</li> <li>The only possible dependence between \(A\) and \(B\) comes from the residual correlation \(\gamma(C).\)</li> </ul> <p>Now define:</p> <ul> <li>Under \(H_0\): \(\gamma(C) = 0\)</li> <li>Under \(H_1\): \(\gamma(C) = \sin(C)\)</li> </ul> <p>So under the alternative, the residual correlation oscillates smoothly as a function of \(C\).</p> <p>Why this is subtle?</p> <p>The <em>marginal</em> residual correlation is</p> \[\mathbb{E}[\gamma(C)] = \mathbb{E}[\sin(C)].\] <p>Since \(C \sim \mathcal{N}(0,1)\) and \(\sin(\cdot)\) is symmetric,</p> \[\mathbb{E}[\sin(C)] = 0.\] <p>So globally, the residual correlation averages out.</p> <p>Marginally, there is no detectable correlation.</p> <p>The dependence only appears <em>locally</em> in regions of \(C\).</p> <div class="row justify-content-center mt-3"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/hardness/residuals_L-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/hardness/residuals_L-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/hardness/residuals_L-1400.webp"/> <img src="/assets/img/blog/hardness/residuals_L.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Marginal and conditional covariance of A and B given C. </div> <p>This is the key difficulty:</p> <blockquote> <p>Conditional dependence may oscillate, cancel globally, and only be visible at the right scale.</p> </blockquote> <ul> <li>If a test averages too aggressively over \(C\), it will miss this structure entirely.</li> <li>If it localizes too aggressively, estimates become very unstable and noisy.</li> </ul> <p>Methods that only measure <em>averaged</em> conditional dependence can completely miss structured, localized interactions. For instance, the Generalized Covariance Measure (GCM), which effectively averages conditional covariance over \(C\), can fail when dependence oscillates and cancels globally <d-cite key="shah2018hardness"></d-cite>.</p> <hr/> <p>The tension discussed above already makes detecting subtle dependence fragile.</p> <p>But there is an even deeper difficulty.</p> <p>Even if we choose the right scale to detect dependence, CI testing can still fail ‚Äî because of how we estimate conditional means.</p> <p>If we look at the wrong structure, we may neglect real dependence and fail to detect it.</p> <p>If the conditional means are estimated inaccurately, we may introduce artificial residual correlation and falsely conclude that dependence exists.</p> <h3 id="when-conditional-means-are-perfect">When conditional means are perfect</h3> <p>To understand where things go wrong in practice, let‚Äôs first look at the idealized setting ‚Äî where we know the conditional means exactly.</p> <p>When linear kernels are used for variables A and B, i.e., \(\phi_A(A)=A\) and \(\phi_B(B)=B\), the Kernel-based Conditional Independence (KCI) can be understood in three conceptual steps.</p> <ol> <li>Get the perfect conditional means: \(\mu_{A|C}(c) = \mathbb{E}[A \mid C=c], \qquad \mu_{B|C}(c) = \mathbb{E}[B \mid C=c].\)</li> <li>Form residuals by removing the effect of \(C\): \(R_A = A - \mu_{A|C}(C), \qquad R_B = B - \mu_{B|C}(C).\)</li> <li>Measure whether the residuals are still dependent, using a kernel on \(C\) to localize the comparison across different regions of the conditioning variable.</li> </ol> <p>Intuitively, KCI asks: after removing everything that can be explained by \(C\), is there any remaining dependence between \(A\) and \(B\)?</p> <p>In this ideal infinite-sample regime, where the conditional means are known perfectly:</p> <ul> <li> <p>Under \(H_0\), \(\text{Cov}(R_A, R_B \mid C) = 0\) almost surely, so the KCI statistic converges to zero.</p> </li> <li> <p>Under \(H_1\), residual dependence remains for some values of \(C\), and the kernel aggregation detects it.</p> </li> </ul> <p>In this idealized setting:</p> <ul> <li>Type I error is controlled, because under the null the population statistic is exactly zero.</li> <li>Type II error depends only on how subtle the remaining dependence is, and whether the chosen kernel can resolve the relevant structure.</li> </ul> <p>In other words, if the conditional means were known exactly, CI testing would be a well-behaved problem.</p> <p>The real trouble begins when we have to estimate those conditional means from data.</p> <h3 id="when-we-have-to-estimate-the-conditional-means">When we have to estimate the conditional means</h3> <p>Everything above assumed we knew the true conditional means.</p> <p>In practice, we do not.</p> <p>We estimate them from data, typically using kernel ridge regression <d-cite key="pogodin2024splitkci"></d-cite>.</p> <p>Write the estimators as</p> \[\hat{\mu}_{A|C}(c) = \mu_{A|C}(c) + \delta_{A|C}(c),\] \[\hat{\mu}_{B|C}(c) = \mu_{B|C}(c) + \delta_{B|C}(c),\] <p>where \(\delta_{A\mid C}\) and \(\delta_{B\mid C}\) are the regression errors.</p> <p>What happens to the residuals?</p> <p>The empirical residuals are now</p> \[\hat{R}_A = A - \hat{\mu}_{A|C}(C), \qquad \hat{R}_B = B - \hat{\mu}_{B|C}(C).\] <p>Substituting,</p> \[\hat{R}_A = (A - \mu_{A|C}(C)) - \delta_{A|C}(C),\] \[\hat{R}_B = (B - \mu_{B|C}(C)) - \delta_{B|C}(C).\] <p>Under our generative model,</p> \[A - \mu_{A|C}(C) = r_A, \qquad B - \mu_{B|C}(C) = r_B.\] <p>So</p> \[\hat{R}_A = r_A - \delta_{A|C}(C), \qquad \hat{R}_B = r_B - \delta_{B|C}(C).\] <p>Now multiply:</p> \[\hat{R}_A \hat{R}_B = r_A r_B - r_A \delta_{B|C}(C) - r_B \delta_{A|C}(C) + \delta_{A|C}(C)\delta_{B|C}(C).\] <p>Taking conditional expectation given \(C\) (and assuming regression is trained on independent data so errors are fixed w.r.t. test sample),</p> \[\mathbb{E}[\hat{R}_A \hat{R}_B \mid C] = \mathbb{E}[r_A r_B \mid C] + \delta_{A|C}(C)\delta_{B|C}(C).\] <p>But</p> \[\mathbb{E}[r_A r_B \mid C] = \gamma(C).\] <p>So we obtain</p> \[\mathbb{E}[\hat{R}_A \hat{R}_B \mid C] = \gamma(C) + \delta_{A|C}(C)\delta_{B|C}(C).\] <p><strong>Under \(H_0\),</strong> \(\gamma(C) = 0.\) So the population residual covariance becomes</p> \[\mathbb{E}[\hat{R}_A \hat{R}_B \mid C] = \delta_{A\mid C}(C)\delta_{B\mid C}(C).\] <p>Even though \(A \perp B \mid C\) holds in truth.</p> <p>When KCI aggregates over \(C\) using the kernel,</p> \[\text{KCI} = \mathbb{E} \left[ k_C(C,C') \, \delta_{A\mid C}(C) \delta_{A\mid C}(C') \delta_{B\mid C}(C) \delta_{B\mid C}(C') \right].\] <p>So the regression errors induce a nonzero population statistic which leads to inflated Type I error.</p> <p>This is not sampling noise.</p> <p>It is structural bias introduced by imperfect regression.</p> <h2 id="type-i-error-inflation-and-type-iii-tradeoff">Type I error inflation and type I/II tradeoff</h2> <h3 id="why-type-i-error-explodes">Why type I error explodes</h3> <p>Under ideal conditions (perfect regression), the KCI statistic behaves like a degenerate U-statistic under the null.</p> <p>Because the population statistic is exactly zero under \(H_0\), the first-order term of the U-statistic vanishes. This degeneracy forces the variance to decay at rate \(1/n\).</p> <p>Null approximations ‚Äî whether chi-square mixtures, Gamma approximations, or wild bootstrap ‚Äî rely critically on this structure.</p> <p>They assume:</p> <ul> <li>The population mean under \(H_0\) is zero.</li> <li>The statistic is asymptotically centered.</li> <li>The dominant stochastic fluctuation is of order \(1/n\) around zero.</li> </ul> <p>When conditional means are estimated imperfectly.</p> <p>Under \(H_0\), the statistic no longer has zero population mean:</p> <p>Because once the statistic has nonzero mean, the U-statistic is no longer degenerate. Its leading term behaves like an empirical average of nonzero quantities.</p> <p>Formally:</p> <ul> <li>The bias term does not vanish with \(n\).</li> <li>The centered fluctuations around this bias are of order \(1/\sqrt{n}\).</li> </ul> <p>So instead of shrinking toward zero, the statistic concentrates around a positive constant:</p> \[\text{KCI}_n = \text{Bias} + O_p(1/\sqrt{n}).\] <p>Unless the regression error itself shrinks sufficiently fast, the bias remains.</p> <table> <thead> <tr> <th>Case</th> <th>Mean of KCI</th> <th>Std Dev</th> </tr> </thead> <tbody> <tr> <td>Perfect regression</td> <td>0</td> <td>\(O(1/n)\)</td> </tr> </tbody> <tbody> <tr> <td>Imperfect regression</td> <td>\(O(1)\)</td> <td>\(O(1/\sqrt{n})\)</td> </tr> </tbody> </table> <p><strong>The consequence:</strong></p> <p>Null calibration procedures still assume a centered statistic.</p> <p>But the true distribution is shifted.</p> <p>As test sample size \(n\) grows:</p> <ul> <li>The variance shrinks.</li> <li>The bias does not.</li> </ul> <p>Eventually, the statistic will almost surely exceed any fixed null threshold.</p> <p>Type I error thus inflate with increasing \(n\).</p> <p>This is why regression error is not a small nuisance.</p> <p>It fundamentally changes the asymptotic regime.</p> <h3 id="type-i-and-type-ii-error-tradeoff">Type I and type II error tradeoff</h3> <p>In principle, we choose the kernel (especially the bandwidth on \(C\)) to maximize power ‚Äî that is, to better detect conditional dependence. However,</p> <blockquote> <p>The same kernel choice that amplifies true dependence can also amplify regression-induced bias.</p> </blockquote> <p>Recall that under imperfect regression, the null statistic contains the term</p> \[\delta_{A\mid C}(C)\,\delta_{B\mid C}(C).\] <p>These regression errors are not arbitrary noise. They are smooth, structured functions of \(C\).</p> <p>When we optimize the kernel bandwidth to increase sensitivity to dependence, we are effectively choosing a weighting function over \(C\).</p> <p>If that weighting aligns with regions where \(\delta_{A\mid C}(C)\,\delta_{B\mid C}(C)\) is large, the test statistic increases ‚Äî even under the null.</p> <p>In other words:</p> <ul> <li>When we tune the kernel to detect real structure,</li> <li>We may instead be tuning it to highlight structured regression error.</li> </ul> <div class="row justify-content-center mt-3"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/hardness/type1_and_type2_error_compare_num-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/hardness/type1_and_type2_error_compare_num-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/hardness/type1_and_type2_error_compare_num-1400.webp"/> <img src="/assets/img/blog/hardness/type1_and_type2_error_compare_num.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Type I and type II error tradeoff when selecting bandwidth for kernel C. Smaller training size corresponds to worse conditional mean estimates. </div> <p>This creates a fundamental tradeoff:</p> <ul> <li>A wider kernel smooths aggressively ‚Üí lower Type I error, but reduced power.</li> <li>A narrower kernel localizes aggressively ‚Üí lower Type II error, but increased risk of Type I inflation.</li> </ul> <p>Optimizing for power can therefore push us directly into spurious rejection.</p> <p>Conditional independence testing is fragile not only because dependence may be subtle,<br/> but because the very act of searching for it can create it.</p> <h3 id="the-central-lesson">The central lesson</h3> <p>CI testing does not merely require detecting dependence. It also requires estimating conditional means accurately enough.</p> <p>That is a very strong requirement.</p> <p>And that is why CI testing fails in practice.</p> <h2 id="final-summary">Final Summary</h2> <h3 id="takeaways-why-ci-is-hard-both-theoretically-and-practically">Takeaways: why CI is hard both theoretically and practically</h3> <p>Conditional independence testing is difficult for structural reasons:</p> <ol> <li> <p><strong>Dependence can hide in subtle structure.</strong><br/> Conditional dependence may be localized, oscillatory, or globally canceling ‚Äî detectable only at the right scale.</p> </li> <li> <p><strong>Regression error induces spurious dependence.</strong><br/> Imperfect estimation of \(\mathbb{E}[\phi_A(A) \mid C]\) and \(\mathbb{E}[\phi_B(B) \mid C]\) introduces artificial residual correlation, even under the null.</p> </li> <li> <p><strong>Detection is unstable in both directions.</strong><br/> Conditional dependence is not only hard to detect (high Type II error), it is also easy to hallucinate when conditional means are estimated inaccurately (inflated Type I error).</p> </li> </ol> <p>Moreover, these phenomena are not specific to KCI. They arise in essentially all conditional dependence measures that rely on estimating conditional expectations, and aggregate residual covariances across values of \(C\).</p> <h3 id="practical-recommendations">Practical recommendations:</h3> <p>Though the theory is pessimistic, CI testing can still be useful in practice ‚Äî if we treat it as a fragile procedure and design the pipeline accordingly.</p> <ul> <li> <p><strong>Sample splitting.</strong><br/> Use an independent training set to estimate the conditional means \(\hat{\mu}_{A|C}\) and \(\hat{\mu}_{B|C}\), and a separate test set to compute the KCI statistic and calibrate the null.<br/> As a rule of thumb, the training set should be at least as large as the test set, and preferably much larger (how much larger depends on the complexity of \(\mathbb{E}[A|C]\) and \(\mathbb{E}[B|C]\)) <d-cite key="pogodin2024splitkci"></d-cite>.</p> </li> <li> <p><strong>Strong regression (bias matters more than you think).</strong><br/> Use flexible, low-bias regression models for conditional mean estimation. CI testing is extremely sensitive to systematic regression error: even small structured bias can look like conditional dependence under \(H_0\).</p> </li> <li> <p><strong>Kernel choice for power (but only on the training split).</strong><br/> If you tune the conditioning kernel \(k_C\) (e.g., its bandwidth) to improve power, do it using only the training split‚Äîe.g., by maximizing an estimated signal-to-noise ratio (SNR). Then <em>freeze</em> the choice and evaluate on the test split.</p> </li> <li> <p><strong>Be cautious about ‚Äúdiscoveries.‚Äù</strong><br/> Even with all safeguards, it is still easy to trick yourself: power tuning can overfit to regression artifacts, and null calibration can silently fail when regression error does not decay fast enough. Treat borderline rejections with skepticism and stress-test them with sensitivity checks.</p> </li> </ul> <h2 id="citation">Citation</h2> <p>If you find this blog helpful, please consider cite</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{hehardness2025,
  title={On the Hardness of Conditional Independence Testing In Practice},
  author={He, Zheng and Pogodin, Roman and Li, Yazhe and Deka, Namrata and Gretton, Arthur and Sutherland, Danica J},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
  year={2025}
}
</code></pre></div></div> <p>PaperÔºö <a href="https://arxiv.org/abs/2512.14000">On the Hardness of Conditional Independence Testing In Practice</a></p> <p>CodeÔºö <a href="https://github.com/he-zh/kci-hardness">github.com/he-zh/kci-hardness</a></p>]]></content><author><name>Zheng HE</name></author><category term="research"/><category term="paper-digest"/><category term="kernel"/><category term="CI"/><category term="testing"/><summary type="html"><![CDATA[Conditional independence (CI) testing is widely used in causal discovery, scientific modeling, fairness, domain generalization, and robustness analysis. And yet it often fails in practice.]]></summary></entry><entry><title type="html">A note on kernel methods</title><link href="https://he-zh.github.io/blog/2024/kernel/" rel="alternate" type="text/html" title="A note on kernel methods"/><published>2024-11-01T00:00:00+00:00</published><updated>2024-11-01T00:00:00+00:00</updated><id>https://he-zh.github.io/blog/2024/kernel</id><content type="html" xml:base="https://he-zh.github.io/blog/2024/kernel/"><![CDATA[<p>Kernel methods are one of the most elegant bridges between linear algebra, functional analysis, and modern machine learning. Once you see the pattern‚Äî<em>map into a Hilbert space, do linear operations there</em>‚Äîeverything from SVMs to distribution comparison becomes a special case.</p> <hr/> <h3 id="1-preliminaries-the-geometry-of-functions">1. Preliminaries: The Geometry of Functions</h3> <p>To understand kernels, we first need to define the spaces in which they live. We transition from finite-dimensional vectors to infinite-dimensional function spaces.</p> <h4 id="11-function-spaces">1.1 Function Spaces</h4> <p>A function space \(\mathcal{F}\) is a vector space where the ‚Äúpoints‚Äù are functions.</p> <p><strong>Definition.</strong> Let \(f, f' \in \mathcal{F}\) correspond to weight vectors \(w, w'\). The operations are defined point-wis</p> <ul> <li>Addition/Scaling: \((\alpha f + f')(x) = \alpha f(x) + f'(x)\).</li> <li>Inner Product: \(\langle f, f' \rangle = w \cdot w'\).</li> <li>Norm: \(\|f\|_{\mathcal{F}} = \sqrt{\langle f, f \rangle} = \|w\|\).</li> </ul> <h4 id="12-the-l2x-mu-space">1.2 The \(L^2(X, \mu)\) Space</h4> <p>In probability and measure theory, we often care about functions whose ‚Äúsize‚Äù is finite under an integral.</p> <p><strong>Definition.</strong> \(L^2(X, \mu)\) consists of all measurable functions \(f\) such that:</p> \[\|f\|_2 = \left( \int_X |f(x)|^2 d\mu(x) \right)^{1/2} &lt; \infty\] <h4 id="13-hilbert-space">1.3 Hilbert Space</h4> <p>A Hilbert space is essentially a vector space equipped with an inner product that is complete (meaning all Cauchy sequences converge within the space).</p> <p>Cauchy Sequence: For every \(\epsilon &gt; 0\), there exists \(N\) such that \(\|f_n - f_m\|_{\mathcal{F}} &lt; \epsilon\) for all \(m, n \ge N\).</p> <p>Completeness: Ensures we can perform calculus and optimization without ‚Äúfalling out‚Äù of the space.</p> <hr/> <h3 id="2-kernels-and-the-rkhs">2. Kernels and the RKHS</h3> <p>The ‚ÄúKernel Trick‚Äù allows us to compute inner products in high-dimensional feature spaces without ever explicitly computing the coordinates of the data.</p> <h4 id="21-what-is-a-kernel">2.1 What is a Kernel?</h4> <p><strong>Definition.</strong> A function \(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) is a positive definite kernel if there exists a Hilbert space \(\mathcal{H}\) and a feature map \(\phi(x): \mathcal{X} \to \mathcal{H}\) such that:</p> \[k(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}\] <p>The Mercer Condition: For \(k\) to be a valid kernel, the kernel matrix \(K\) (where \(K_{ij} = k(x_i, x_j)\)) must be Symmetric Positive Semi-Definite (SPSD).</p> <p>This means: \(\forall \alpha \in \mathbb{R}^m, \alpha^\top K \alpha \ge 0\).</p> <p>All eigenvalues of \(K\) are non-negative.</p> <p>In machine learning, commonly used kernels include the Gaussian and Laplace kernels, i.e.,</p> \[k(x,x')=\exp\left(-\frac{\| X-x'\|^2_2}{2\sigma^2}\right), \qquad k(x,x')=\exp\left(-\frac{\| X-x'\|_1}{\sigma}\right)\] <p>where \(\sigma &gt; 0\) is a bandwidth parameter. These kernels belong to a class of kernel functions called radial basis functions (RBF).</p> <p><strong>Universal and characteristic kernel</strong> <a href="https://jmlr.org/papers/volume12/sriperumbudur11a/sriperumbudur11a.pdf">(reference)</a></p> <p>Universal and characteristic‚Äîhave been developing in parallel in machine learning: universal kernels are proposed in the context of achieving the Bayes risk by kernel-based classification/regression algorithms while characteristic kernels are introduced in the context of distinguishing probability measures by embedding them into a reproducing kernel Hilbert space (RKHS).</p> <h4 id="22-reproducing-kernel-hilbert-space-rkhs">2.2 Reproducing Kernel Hilbert Space (RKHS)</h4> <p>The RKHS is a special Hilbert space where evaluation is a bounded linear functional.</p> <p><strong>Evaluation Functional</strong>:</p> <ul> <li>For each \(x\in X\), the evaluation functional \(Lx\)‚Äã is a map that takes a function \(f‚ààH\) and returns its value at ùë•: \(Lx‚Äã(f)=f(x)\)</li> <li>This functional ùêøùë•‚Äã is linear, meaning: \(Lx‚Äã(f+g)=Lx‚Äã(f)+Lx‚Äã(g)\) and \(Lx‚Äã(Œ±f)=Œ±Lx‚Äã(f)\)</li> </ul> <p><strong>Definition (Reproducing kernel Hilbert space RKHS).</strong> A Hilbert space \(\mathcal{H}\) of functions is a reproducing kernel Hilbert space (RKHS) if the evaluation functionals \(F_x[f]\) defined as \(F_x[f] = f(x)\) are bounded, i.e., for all \(x ‚àà X\) there exists some \(C &gt; 0\) such that</p> \[\|F_x[f]\|=f(x)\le C\|f\|_{\mathcal{H}}, \qquad \forall f \in \mathcal{H}\] <p>Interpretation: In many spaces, two functions can be ‚Äúclose‚Äù in norm but have very different values at a specific point \(X\). In an RKHS, if functions are close in norm, their values \(f(x)\) are also close.</p> <p><strong>The Reproducing Property:</strong> For every \(x \in X\), there exists a function \(k_x \in \mathcal{H}\) (the representer of evaluation) such that:</p> \[f(x) = \langle f, k_x \rangle_{\mathcal{H}}\] <p>If we set \(f = k_y\) (or written as \(k(\cdot, y)\)), we get the kernel itself:</p> \[k(x, y) = \langle k_x, k_y \rangle_{\mathcal{H}}\] <p>The RKHS \(\mathcal{H}\) is fully characterized by the reproducing kernel \(k\).</p> <p><strong>Theorem.</strong> For every positive definite function \(k(¬∑, ¬∑)\) on \(X √ó X\) there exists a unique (up to isometric isomorphism) RKHS with \(k\) as its reproducing kernel. Conversely, the reproducing kernel of an RKHS is unique and positive definite.</p> <p><strong>The kernel k generates the RKHS, i.e. \(\mathcal{H}=\text{span}\{k(x,\cdot)\mid x\in X\}\)</strong></p> <p>define \(\phi(x) := k(x,\cdot)=\{x' \to k(x,x')\}\) for all \(X\). \(\mathcal{H}\) is the set of all linear combinations of these functions \(\sum_{i=1}^m \alpha_i \phi(x_i)\) for all \(x_1, \dots, x_m \in \mathcal{X}\), \(\alpha_1, \dots, \alpha_m \in \mathbb{R}\).</p> <p>Also, we have</p> \[\left\langle \sum_{i=1}^m \alpha_i \phi(x_i) , \sum_{j=1}^m \beta_j \phi(x'_j)\right\rangle_\mathcal{H} = \sum_{i=1}^m \sum_{j=1}^m \alpha_i \beta_j K(x_i, x_j')\] <p>By virtue of reproducing property, we know</p> <ul> <li>\(k(x, ¬∑)\) is a high-dimensional representer of \(X\)</li> <li>by the reproducing property it also acts as a representer of evaluation of any function in \(\mathcal{H}\) on the data point \(X\)</li> </ul> <p><strong>Definition (Separable Hilbert Space).</strong> A Hilbert space \(\mathcal{H}\) is said to be separable if it has a countable basis.</p> <p>If \(a\in \mathcal{H}\) and \((e_i)_{i\in I}\) is an orthonormal basis for \(\mathcal{H}\). The index sets \(I\) are assumed to be either finite or countably infinite. Then</p> \[a = \sum_{i‚ààI} \langle a, e_i\rangle_{\mathcal{H}} e_i .\] <p>Most we dealt with is separable. example of non-separable \(k(x,y)=\mathbb{(x=y)}\)</p> <h5 id="221-tensor-product-reference">2.2.1 Tensor Product <a href="https://www.uio.no/studier/emner/matnat/math/nedlagte-emner/MAT-INF2360/v12/tensortheory.pdf">(reference)</a></h5> <p><strong>Definition(Tensor product of vectors).</strong><br/> If x, y are vectors of length M and N, respectively, their tensor product \(x‚äóy\) is defined as the \(M √óN\)-matrix defined by \((x ‚äó y)_{ij} = x_iy_j\) . In other words, \(x ‚äó y = xy^T\).</p> <p><strong>Definition 7.3 (Tensor product of matrices).</strong> If \(S : R^M ‚Üí R^M\) and \(T : R^N ‚Üí R^N\) are matrices, we define the linear mapping \(S ‚äó T : L_{M,N}(R) ‚Üí L_{M,N}(R)\) by linear extension of \((S ‚äó T)(e_i ‚äó e_j )=(Se_i) ‚äó (Te_j )\). The linear mapping \(S ‚äó T\) is called the tensor product of the matrices S and T.</p> <p><strong>Tensor product</strong></p> <ul> <li>\(\langle f, k_X \rangle\) \(\langle g, l_Y\rangle = \langle f, k_X‚äól_Y g \rangle\)</li> <li>\((a‚äób) x = \langle x, b\rangle a\), \((ba^T)f=(a^Tf)b\)</li> <li>\(\langle g\otimes f , \phi(Y) \otimes \psi(X) \rangle_{\mathcal{G} \otimes \mathcal{H}}\)=\(\langle g, \psi(Y)\rangle_{\mathcal{G}} \langle \psi(X),f\rangle_{\mathcal{H}}\)</li> <li>\((S\otimes T)(x\otimes y)\) = \((Sx)\otimes(Ty)\)</li> <li>\((S_1 ‚äó T_1)(S_2 ‚äó T_2)\) = \((S1S2) ‚äó (T_1T_2)\)</li> <li>\((S ‚äó T)X = SXT^T\) if S, T are matrices</li> </ul> <h5 id="222-hilbert-schmidt-operator-reference">2.2.2 Hilbert-Schmidt Operator <a href="https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture5_covarianceOperator.pdf">(reference)</a></h5> <p><strong>Hilbert-Schmidt operator.</strong> Let \(\mathcal{F}\) and \(\mathcal{G}\) be separable Hilbert spaces and \((f_i)_{i‚ààI}\) and \((g_j)_{j‚ààJ}\) are orthonormal basis for \(\mathcal{F}\) and \(\mathcal{G}\), respectively. A Hilbert-Schmidt operator is a <em>bounded</em> operator \(\mathcal{A}: \mathcal{G}\to\mathcal{F}\) whose Hilbert-Schmidt norm</p> \[\|\mathcal{A}\|_{HS}^2 = \sum_{j\in J} \|\mathcal{A}g_j\|_{\mathcal{F}}^2=\sum_{i\in I}\sum_{j\in J} |\langle \mathcal{A} g_j, f_i\rangle_\mathcal{F}|^2\] <p>is <em>finite</em> (\(\|\mathcal{A}\|_{HS}^2 &lt; \infty\)).</p> <p>The Hilbert-Schmidt operators mapping from \(\mathcal{G}\) to \(\mathcal{F}\) form a Hilbert space, written \(HS(\mathcal{G},\mathcal{F})\), with inner product</p> \[\langle L, M\rangle_{HS} = \sum_{j\in J} \langle L g_j, M g_j\rangle_\mathcal{F}=\sum_{i\in I}\sum_{j \in J} \langle L g_i, f_j\rangle_\mathcal{F}\langle L g_i, M f_j\rangle_\mathcal{F}\] <p>Given \(b ‚àà \mathcal{G}\) and \(a ‚àà \mathcal{F}\), we define the tensor product \(a‚äób\) as a rank-one operator from \(\mathcal{G}\) to \(\mathcal{F}\),</p> \[(a\otimes b)g \mapsto \langle g,b \rangle_\mathcal{G} a\] <p>The Hilbert-Schmidt norm is</p> \[\begin{align*} \|a \otimes b\|_{HS}^2 &amp;= \sum_{j\in J} \|(a \otimes b) g_j \|_{\mathcal{F}}^2\\ &amp;= \sum_{j\in J} \|\langle g_j ,b \rangle_\mathcal{G} a \|_{\mathcal{F}}^2\\ &amp;= \sum_{j\in J} |\langle g_j ,b \rangle_\mathcal{G}|^2 \| a \|_\mathcal{F}^2\\ &amp;= \|b\|_\mathcal{G}^2 \|a\|_{\mathcal{F}}^2 \end{align*}\] <p><strong>Definition (bounded operator).</strong><br/> A linear operator \(A : F ‚Üí R\) is bounded when</p> \[Af \le \lambda_A \|f\|_\mathcal{F} \quad \forall f \in \mathcal{F}\] <p><strong>Property.</strong></p> <ul> <li>\(\langle L, a\otimes b \rangle_{HS}=\langle a, Lb \rangle_\mathcal{F}\), given \(a\otimes b, L ‚àà HS(\mathcal{G}, \mathcal{F})\),</li> <li>\(\langle u\otimes v, a\otimes b \rangle_{HS}=\langle u, a \rangle_\mathcal{F} \langle b, v \rangle_\mathcal{G}\),</li> <li>\(\|\phi(x) \otimes \psi(y)\|_{HS} = \|\phi(x)\|_{\mathcal{F}} \|\psi(y)\|_{\mathcal{G}}=\sqrt{k(x,x) l(y,y)}\),</li> <li>\(\langle C_{XY}, f\otimes g\rangle_{HS} = E_{xy}\langle \phi(x)\otimes \psi(y), f\otimes g\rangle_{HS} = E_{xy} [\langle f, \phi(x)\rangle_{\mathcal{F}}, \langle g, \psi(y)\rangle_{\mathcal{G}}]=E_{xy}[f(x)g(y)]\),</li> <li>\(\langle C_{XY}, C_{XY}\rangle_{HS} = E_{x,y}E_{x',y'}k(x,x')l(y,y')\),</li> <li>\(\langle \mu_{X} \otimes \mu_{Y}, \mu_{X} \otimes \mu_{Y} \rangle_{HS} = E_{x,x'}E_{y,y'}k(x,x')l(y,y')\),</li> <li>In vector space, \(\langle C_{XY}, A\rangle_{HS} = trace(C_{XY}^\top A)=\sum_j (C_{XY}g_j)^\top (Ag_j)\).</li> <li>\(\|a \otimes b\|_{HS} = \|a\|_{\mathcal{F}}^2 \|b\|_{\mathcal{G}}^2\).</li> </ul> <hr/> <h3 id="3-kernel-mean-embeddings">3. Kernel Mean Embeddings</h3> <p>Kernel mean embeddings allow us to represent entire probability distributions as a single point (a function) in an RKHS.</p> <p><strong>Definition.</strong> The mean embedding of a distribution \(\mathbb{P}\) is:</p> \[\mu_{\mathbb{P}} := \mathbb{E}_{X \sim \mathbb{P}} [k(X, \cdot)] = \int_{\mathcal{X}} \phi(x) d\mathbb{P}(x)\] <p>Why is this useful?</p> <ul> <li>If the kernel is ‚Äúcharacteristic‚Äù (like Gaussian or Laplace), the mapping \(\mathbb{P} \to \mu_{\mathbb{P}}\) is injective. This means \(\mu_{\mathbb{P}}\) contains all information about the distribution. This means that \(\|{\mu_\mathbb{P}-\mu_\mathbb{Q}}\|\) if and only if \(\mathbb{P}=\mathbb{Q}\).</li> <li>If \(\mathbb{P}\) and \(\mathbb{Q}\) are close in probability distance measures, then \(\mu_\mathbb{P}\) is also close to \(\mu_\mathbb{Q}\) in the \(\|\cdot \|_{\mathcal{H}}\) norm</li> </ul> <p>**Expectations as Inner Products: ** To calculate the expected value of a function \(f\), we simply take an inner product:</p> \[\mathbb{E}_{\mathbb{P}}[f(x)] = \langle f, \mu_{\mathbb{P}} \rangle_{\mathcal{H}}\] <p><strong>Lemma (<a href="https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture5_covarianceOperator.pdf">Existence of mean embedding</a>).</strong> If \(\mathbb{E}_{X‚àº\mathbb{P}}[\sqrt{k(X, X)}] &lt; \infty\), then \(\mu_\mathbb{P}\in\mathcal{H}\) and \(\mathbb{E}_{X‚àº\mathbb{P}}[f(X)]=‚ü®f,\mu_{\mathbb{P}}‚ü©_\mathcal{H}\).</p> <hr/> <h3 id="4-operators-covariance-and-conditional-mean">4. Operators: Covariance and Conditional Mean</h3> <p>Just as we have covariance matrices for vectors, we have operators for functions.</p> <h4 id="41-cross-covariance-operator">4.1 Cross-Covariance Operator</h4> <p><strong>Definition.</strong> The uncentered cross-covariance operator \(C_{YX}: \mathcal{H} \to \mathcal{G}\) is:</p> \[C_{YX} := \mathbb{E}_{YX} [\phi(Y) \otimes \phi(X)]\] <p>It satisfies the property:</p> <p>\(\langle g, C_{YX} f \rangle_{\mathcal{G}} = \text{Cov}(g(Y), f(X))\).</p> <p>Centered cross-covariance operator is</p> \[\tilde C_{YX} :=\mathbb{E}_{YX}[œÜ(Y)‚äó\phi(X)] - \mu_{P_Y} ‚äó \mu_{P_X}=Œº_{P_{YX}} - \mu_{P_Y ‚äó P_X},\] <p>Equivalently, we can define an operator \(C_{YX}\) as a unique bounded operator that satisfies</p> \[\langle g, C_{YX}f\rangle = Cov[g(Y),f(X)]\] <p>for all \(f \in \mathcal{H}\) and \(g \in \mathcal{G}\).</p> <p><strong>Covariance operator.</strong> If \(X = Y\) , we call \(C_{XX}\) the covariance operator.</p> <ul> <li>\((C_{YX}f)(\cdot)=\int_{X\times Y}l(\cdot,y)f(x) dP_{XY}(x,y)\).</li> <li>\((C_{XX}f)(\cdot)=\int_{X}k(\cdot,x)f(x) dP_{X}(x)\).</li> </ul> <p><strong>Theorem.</strong> If \(\mathbb{E}_{YX}[g(Y)\mid X=¬∑]‚àà \mathcal{H}\) for \(g‚àà\mathcal{G}\), then</p> \[C_{XX}\mathbb{E}_{Y\mid X}[g(Y)\mid X =¬∑]=C_{XY} g\] <details><summary>Proof for the theorem</summary> \[\begin{align*} C_{XX}\mathbb{E}_{Y\mid X}[g(Y)\mid X =¬∑] &amp;= \mathbb{E}_{XX}[\phi(X)‚äó\phi(X)] \mathbb{E}_{Y\mid X}[g(Y)\mid X =¬∑]\\ &amp;= \int_{X}\int_Y \phi(x)\otimes\phi(x) \langle g , \varphi(y)\rangle_{\mathcal{G}} dP_X(x)dP_{Y\mid X}(y\mid X) \\ &amp;= \int_X\int_Y \phi(x)\otimes\phi(x) \langle g , \varphi(y)\rangle_{\mathcal{G}} dP_{XY}(x,y) \\ \end{align*}\] </details> <p><strong>Empirical estimate of the centered \(\tilde C_{YX}\):</strong></p> \[\begin{align*} \hat C_{YX}&amp;= \frac{1}{n} \sum_{i=1}^{n} l(y_i,\cdot)\otimes k(x_i,\cdot) - \hat \mu_{P_Y} \otimes \hat \mu_{P_X} \\ &amp;=\frac{1}{n} \sum_{i=1}^{n} \{l(y_i,\cdot)-\hat \mu_{P_Y} \} \otimes \{k(x_i,\cdot)-\hat\mu_{P_X} \} \\ &amp;= \frac1n (\Psi - \hat\mu_{P_Y} \mathbf{1}^\top)(\Phi - \hat\mu_{P_Y} \mathbf{1}^\top)^\top \\ &amp;= \frac1n (\Psi (I-\frac1n \mathbf{1} \mathbf{1}^\top))(\Phi (I-\frac1n \mathbf{1} \mathbf{1}^\top))^\top \\ &amp;= \frac1n \Psi (I-\frac1n \mathbf{1} \mathbf{1}^\top) \Phi^\top \\ &amp;=\frac1n \Psi H \Phi^\top \\ \end{align*}\] <p>where \(H = I_n ‚àí \frac1n \mathbb{1}_n\) is the centering matrix with \(\mathbb{1}_n\) an n √ó n matrix of ones, \(\Psi = (œÜ(y_1),...,œÜ(y_n))\) and \(\Phi = (\phi(x_1),...,\phi(x_n))\)</p> <h4 id="42-conditional-mean-embedding-cme">4.2 Conditional Mean Embedding (CME)</h4> <p>The CME represents the conditional distribution \(P(Y\mid X=x)\) as an element in \(\mathcal{G}\).</p> <p>Suppose \(X\) and \(Y\) be measurable spaces and let \(X\) and \(Y\) be random variables taking values in \(X\) and \(Y\) respectively. Assume \(k : X √óX ‚Üí R\) and \(l : Y √ó Y ‚Üí R\) to be positive definite kernels with corresponding RKHS‚Äôs \(H\) and \(G\). Let \(U_{Y\mid X} :H ‚ÜíG\) and \(U_{Y\mid X} ‚ààG\) be conditional mean embeddings of the conditional distribution \(P(Y \mid X)\) and \(P(Y \mid X =x)\)</p> \[U_{Y\mid X} = E_{Y\mid X}[\varphi(Y)\mid X=x] = U_{Y\mid X}k(x,\cdot)\] \[E_{Y\mid X}[g(Y)\mid X=x] = \langle g, U_{Y\mid X} \rangle_\mathcal{G} , \forall g \in \mathcal{G}\] <p><strong>Definition.</strong> Let \(C_{XX} : H ‚Üí H\) and \(C_{YX} : H ‚Üí G\) be the covariance operator of X and cross- covariance operator between X to Y, respectively. Then, the conditional mean embedding \(U_{Y\mid X} and U_{Y\mid X}\) are defined as</p> \[U_{Y\mid X}:= C_{YX} C^{-1}_{XX}\] \[U_{Y\mid X}:=C_{YX}C^{-1}_{XX}k(x,\cdot)\] <p><strong>property</strong></p> <ul> <li>\(E_{Y\mid X}[g(Y)\mid X=x]=\langle g, U_{Y\mid X}\rangle=\langle E_{Y\mid X}[g(Y)\mid X], k(x,\cdot)\rangle\).</li> </ul> <p><strong>Empirical estimate:</strong></p> \[\begin{align*} \hat U_{Y\mid X} &amp;= C_{YX} (C_{XX} + \lambda I)^{-1}\\ &amp;= \Psi (K+\lambda I)^{-1} \Phi^\top\\ \end{align*}\] <p>where \(\Psi:= (\psi(y_1), \dots, \psi(y_n))\), \(\Phi:= (\phi(x_1), \dots, \phi(x_n))\), \(K=\Phi^\top \Phi\) is the Gram matrix for samples from variable X, and \(Œª\) is the regularization parameter to avoid overfitting.</p> \[\begin{align*} \hat U_{Y\mid X} &amp;= \Psi (K+\lambda I)^{-1} \Phi^\top k(x,\cdot)\\ &amp;= \Psi (K+\lambda I)^{-1} K_{:,x} \\ \end{align*}\] <p>where \(K_{:,x} = (k(x,X_1), \dots, k(x,X_n))^\top\)</p> <p>Regression Interpretation: The operator \(U_{Y\mid X} = C_{YX} C_{XX}^{-1}\) is actually the solution to a Vector-Valued Ridge Regression:</p> \[\hat{U}_{Y\mid X} = \arg\min_{U} \sum_{i=1}^n \|\phi(y_i) - U\phi(x_i)\|^2_{\mathcal{G}} + \lambda \|U\|^2_{HS}\] <hr/> <h3 id="references">References</h3> <ul> <li>Sriperumbudur et al. (2011), <em>On the Universality and Characteristic Kernels</em></li> <li>Gretton et al., <em>Notes on Mean Embeddings and Covariance Operators</em></li> <li>Muandet et al. (2017), <em>Kernel Mean Embedding of Distributions: A Review and Beyond</em></li> <li>Song et al. (2013), <a href="https://www.gatsby.ucl.ac.uk/~gretton/papers/SonFukGre13.pdf"><em>Kernel Embeddings of Conditional Distributions</em></a></li> <li>Song et al. (2009), <a href="https://www.ri.cmu.edu/pub_files/2009/6/icml09.pdf"><em>Hilbert Space Embeddings of Conditional Distributions with Applications to Dynamical Systems</em></a></li> </ul>]]></content><author><name></name></author><category term="basics"/><category term="kernel"/><category term="note"/><summary type="html"><![CDATA[for readers who want to have a quick reference to kernels + RKHS basics.]]></summary></entry><entry><title type="html">A note on learning bounds</title><link href="https://he-zh.github.io/blog/2023/learning-bounds-note/" rel="alternate" type="text/html" title="A note on learning bounds"/><published>2023-12-30T00:00:00+00:00</published><updated>2023-12-30T00:00:00+00:00</updated><id>https://he-zh.github.io/blog/2023/learning-bounds-note</id><content type="html" xml:base="https://he-zh.github.io/blog/2023/learning-bounds-note/"><![CDATA[<h1 id="overview">Overview</h1> <p>In the heart of machine learning lies a fundamental question: <strong>can we trust the predictions of our models beyond the data we trained them on?</strong> This is where the analysis of generalization error comes into play.</p> <p>Central to this analysis lies the concept of excess error, a measure of how much a model‚Äôs performance deviates from its ideal, true performance. But this excess error isn‚Äôt a monolithic entity; it‚Äôs a captivating dance between two key players: approximation error and estimation error.</p> \[\underbrace{L_D(\hat h_S) - L_{Bayes}}_\text{excess error} = \underbrace{L_D(\hat h_S) - L_D(h^*)}_\text{estimation error} + \underbrace{L_D(h^*) - L_{Bayes}}_\text{approximation error},\] <p>where \(\hat h_S \in \arg\min_{h\in\mathcal{H}} L_S(h)\), \(h^* = \arg\inf_{h\in\mathcal{H}} L_D(h)\), \(L_S(\cdot)\) is the empirical risk and \(L_D(\cdot)\) is the expected risk.</p> <ul> <li><strong>Excess Error:</strong> \(L_D(\hat h_S) - L_{Bayes}\) is the difference between the error of a model and the Bayes error (Bayes risk or irreducible error) on a given task.</li> <li><strong>Bayes Error:</strong> \(L_{Bayes}\) represents the lowest achievable error rate for any classifier on that task.</li> <li>The excess error in Empirical Risk Minimization (ERM) can be effectively bounded by dissecting it into two primary components: estimation error and approximation error.</li> <li><strong>Estimation Error:</strong> \(L_D(\hat h_S) - L_D(h^*)\) arises from using our algorithm \(\hat h_S\) instead of selecting the best predictor \(h^*\) within the hypothesis space \(\mathcal{H}\). As the sample size m approaches infinity, the estimation error ideally tends toward zero.</li> <li><strong>Approximation Error:</strong> \(L_D(h^*) - L_{Bayes}\) is incurred by choosing the optimal predictor within \(\mathcal{H}\) rather than utilizing the optimal classifier (Bayes classifier) from any hypothesis.</li> </ul> <p>This note delves into various techniques that offer guarantees on how well our models generalize based on uniform or non-uniform convergence, and introduce concepts like VC dimension, covering numbers, Rademacher complexity and stability bound.</p> <h1 id="uniform-convergence">Uniform convergence</h1> <p>Bounding the estimation error of Empirical Risk Minimization (ERM) involves controlling the generalization gap between the empirical risk and the expected risk. The estimation error could be written as</p> \[L_D(\hat h_S) - L_D(h^*) \le L_D(\hat h_S) - L_S(\hat h_S) + L_S(h^*) - L_D(h^*)\] <p>and we can probabilistically bound Generalization gap for \(h^*\) and \(\hat h_S\).</p> <p><strong>Upper bound \(L_S(h^*) - L_D(h^*)\):</strong> Hoeffiding‚Äôs inequality</p> <p>Let \(L_S(h^*)\) be an average of iid random variables and \(L_D(h^*)\) be its expectation, we have</p> \[L_S(h^*) - L_D(h^*) = \frac{1}{m}\sum_{i=1}^{m} \ell(h,z_i) - \mathbb{E}_{z\sim D} \ell (h,z),\] <p>If we further assume that \(\ell(h,z)\in [a, b]\) for all \(h, z\), we could apply <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding‚Äôs inequality</a> to get</p> \[\mathrm{Pr}(L_S(h^*) - L_D(h^*) \le (b-a) \sqrt{\frac{\log 1/\delta}{2m}} ) \ge 1-\delta\] <ul> <li>For the assumption that \(\ell(h,z)\in [a, b]\) <ul> <li>bounded loss naturally satisfies <ul> <li>0-1 loss, ramp loss</li> </ul> </li> <li>unbounded loss might be depending on \(H\) and \(D\) <ul> <li>square loss, logistic loss, hinge loss‚Ä¶</li> </ul> </li> </ul> </li> </ul> <p><strong>Upper bound \(L_D(\hat h_S) - L_S(\hat h_S)\):</strong> uniform convergence</p> <p>We can‚Äôt apply <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding‚Äôs inequality</a> directly on the generalization gap of \(\hat h_S\) as \(\ell(\hat h_S, z_i)\) aren‚Äôt independent. The choice of \(\hat h_S\) depends on all of \(S\), i.e. on all of the other \(z_j\) , as well as the ones we‚Äôre evaluating on.</p> <p><strong>Uniform convergence.</strong> The basic idea is that if we know that \(L_D(h) - L_S(h)\) is small for all \(h\in\mathcal{H}\), then it‚Äôll be small for \(\hat h_S\)</p> \[L_D(\hat h_S) - L_S(\hat h_S) \le \sup_{h\in\mathcal{H}} L_D(h) - L_S(h) ,\] <p>and bound the \(\sup_{h\in\mathcal{H}} L_D(h) - L_S(h)\) instead.</p> <p>Here are a few commonly used techniques:</p> <ul> <li>Finite \(\vert \mathcal{H}\vert &lt;\infty\) <ul> <li>we could use the union bound <ul> <li>but \(\vert \mathcal{H}\vert\) might be really large which makes the bound vacuous</li> </ul> </li> </ul> </li> <li>Infinite \(\vert \mathcal{H}\vert =\infty\) <ul> <li>covering numbers</li> <li>Rademacher complexity</li> <li>VC dimension</li> </ul> </li> </ul> <hr/> <h2 id="finite-hypothesis-class-mathcalhinfty">Finite hypothesis class \(|\mathcal{H}|&lt;\infty\)</h2> <p>With probability at least \(1-\delta\), we have</p> \[\begin{align*}L_D(\hat h_S) - L_D(h^*) &amp;\le \sup_{h\in\mathcal{H}}[ L_D(h) - L_S(h)] + L_S(h^*) - L_D(h^*) \\ &amp;\le (b-a) \sqrt{\frac{2}{m}\log \frac{|H|+1}{\delta}} \end{align*}\] <details><summary>Proof for the finite hypothesis class bound</summary> <p>We start by noting</p> \[\sup_{h\in\mathcal{H}} L_D(h) - L_S(h) \le \varepsilon \leftrightarrow \forall h\in\mathcal{H}. \ L_D(h) - L_S(h) \le \varepsilon\] <p>Use union bound \(P(A+B) \le P(A) + P(B)\) we get</p> \[\mathrm{Pr}_{S\sim D^m}(\exists h\in\mathcal{H}. \ L_S(h) - L_D(h) &gt; \varepsilon) \le \sum_{h\in\mathcal{H}} P_{S\sim D^m} (L_S(h) - L_D(h) &gt; \varepsilon)\] <p>Let \(\mathrm{Pr}_{S\sim D^m} (L_S(h) - L_D(h) &gt; \varepsilon) \le \frac{\delta}{|H|+1}\) for \(\forall h \in\mathcal{H}\), where \(\varepsilon=(b-a) \sqrt{\frac{1}\log \frac{|H|+1}{\delta}}\) then</p> \[\begin{align*} \mathrm{Pr}_{S\sim D^m}(\exists h\in\mathcal{H}. \ L_S(h) - L_D(h) &gt; \varepsilon) \le \frac{|\mathcal{H}|\delta}{|\mathcal{H}|+1}\\ \mathrm{Pr}_{S\sim D^m}(L_S(h^*) - L_D(h^*) &gt; \varepsilon) \le \frac{\delta}{|\mathcal{H}|+1}\end{align*}\] <p>Thus, we have</p> \[\mathrm{Pr}_{S\sim D^m}\left((\sup_{h\in\mathcal{H}} L_S(h) - L_D(h) \le \varepsilon) \bigcap (L_S(h^*) - L_D(h^*) \le \varepsilon)\right) \ge 1- \delta,\] <p>which means with probability at least \(1-\delta\), the following inequality holds</p> \[\sup_{h\in\mathcal{H}}[ L_S(h) - L_D(h)] + L_S(h^*) - L_D(h^*) \le 2\varepsilon\] </details> <p><br/></p> <hr/> <h2 id="infinite-hypothesis-class-mathcalhinfty">Infinite hypothesis class \(|\mathcal{H}|=\infty\)</h2> <h3 id="covering-numbers">Covering numbers</h3> <p>We can rewrite the uniform bound like this</p> \[\begin{align*} &amp;\sup_{h\in\mathcal{H}} L_D(h) - L_S(h) \\ &amp;= \sup_{h\in\mathcal{H}} L_D(h) - L_D(t) + L_D(t) - L_S(t) + L_S(t)- L_S(h) \\ &amp;\le \sup_{h\in\mathcal{H}} [L_D(h) - L_D(t)] + \sup_{t\in\mathcal{T}} [L_D(t) - L_D(t)] + \sup_{h\in\mathcal{H}} [L_S(t)- L_S(h)] \end{align*}\] <p>where \(\mathcal{T}\) is a finite \(œÅ\)-cover set of \(\mathcal{H}\).</p> <p><strong>Definition (\(\rho\)-cover set).</strong> A \(œÅ\)-cover of a set \(\mathcal{H}\) is a set \(\mathcal{T}‚äÜ\mathcal{H}\) such that, for all \(h‚àà\mathcal{H}\), there is a \(t ‚àà \mathcal{T}\) with \(dist(t, h) ‚â§ œÅ\). The smallest size of the \(\rho\)-cover set of \(\mathcal{H}\) is \(N(\mathcal{H}, \rho)\).</p> <p><strong>Bounds with \(\rho\)-covering</strong></p> <ul> <li>To bound the first and the third term, we need to assume <a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuity</a> of loss that \(\vert \ell(h,z) - \ell(t,z)\vert \le K \Vert {h(x)-t(x)}\Vert\). According to the assumption of \(œÅ\)-cover, we also have \(\Vert{h-t}\Vert \le \rho\). Thus, we can bound the differences respectively.</li> <li>To bound the second term, as \(\mathcal{T}\) is finite set, we could apply <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding‚Äôs inequality</a> as the finite hypothesis class case. The only problem is how to decide the size of \(\mathcal{T}\).</li> </ul> <details><summary><strong>Example:</strong> logistic regression with bounded \(X\) and bounded \(\mathcal{H}\)</summary> <ul> <li>Input space \(Z = X \times Y, X = \mathbb{R}^d, \Vert x\Vert \le C, Y=\{-1,1\}\)</li> <li>Hypothesis class \(\mathcal{H}=\{x\to w\cdot x : w\in \mathbb{R}^d, \Vert w\Vert \le B\}\)</li> <li>Logistic loss \(\ell_{\rm{log}} (h,z) =l_y(h(x)) = \log(1+\exp(-y h(x)))\), which is 1-Lipschitz. And we also have \(\vert \ell_{\rm{log}} (h,z)\vert \le |1+h(x)\vert \le 1+BC\) According to Lipschitz continuity, we have \(\vert \ell(h,z) - \ell(t,z)\vert \le \Vert{w\cdot x - t\cdot x}\Vert \le \Vert w-t\Vert \Vert x\Vert = \rho C\). Thus,</li> </ul> \[\begin{align*} \sup_{h\in\mathcal{H}} [L_D(h) - L_D(t)] \le \rho C \\ \sup_{h\in\mathcal{H}} [L_S(h) - L_S(t)] \le \rho C \end{align*}\] <p>Besides, with probability at least \(1-\delta\), we have</p> \[\sup_{h\in\mathcal{T}} L_D(t) - L_S(t) \le (1+BC) \sqrt{\frac{1}{2m}\log\frac{N(\mathcal{H},\rho)}{\delta}} \le \frac{1+BC}{\sqrt{2m}} \left(\sqrt{\log\frac{1}{\delta}} + \sqrt{\log N(\mathcal{H},\rho)}\right)\] <p><strong>The overall bound on estimation error of logistic regression:</strong> from the above, we know that</p> \[\begin{align*} &amp;\mathrm{Pr}\left(\sup_{h\in\mathcal{H}} [L_D(h) - L_S(h)] &gt; 2\rho C + \frac{1+BC}{\sqrt{2m}} (\sqrt{\log\frac{1}{\delta}} + \sqrt{\log N(\mathcal{H},\rho)}) \right) &lt; \delta \\ &amp;\mathrm{Pr}\left(L_S(h^*) - L_D(h^*) &gt; \frac{1+BC}{\sqrt{2m}} \sqrt{\log\frac{1}{\delta}} \right) &lt; \delta \end{align*}\] <p>Thus, with probability at least \(1-\delta\) the following holds</p> \[\begin{align*} L_D(\hat h_S) - L_D(h^*) &amp;\le \sup_{h\in\mathcal{H}} [L_D(h) - L_S(h)] + L_S(h^*) - L_D(h^*)\\ &amp;\le 2\rho C + \frac{1+BC}{\sqrt{2m}} (2\sqrt{\log\frac{2}{\delta}} + \sqrt{\log N(\mathcal{H},\rho)}) \end{align*}\] <p>where \(\rho\) can be further optimized to get rid of \(\rho\) and obtain a tighter bound.</p> </details> <p><br/></p> <hr/> <h3 id="rademacher-complexity">Rademacher complexity</h3> <ul> <li>Rademacher complexity is typically applied on a function class of models that are used for <strong>classification</strong>, with the goal of measuring their ability to classify points drawn from a probability space under arbitrary labellings.</li> <li>When the function class is rich enough, it contains functions that can appropriately adapt for each arrangement of labels, simulated by the random draw of¬†\(\sigma_i\) under the expectation, so that this quantity in the sum is maximized.</li> <li>measures richness of a class of real-valued functions with respect to a¬†probability distribution.</li> </ul> <p><strong>Definition (Rademacher complexity).</strong> The Rademacher complexity of a set \(V ‚äÜ \mathbb{R}^m\) is given by \(\mathrm{Rad}(V) = \mathbb{E}_{\sigma\sim\rm{Unif}(\pm1)^m} \sup_{v\in V} \frac{\sigma \cdot v}{m}\)</p> <p><strong>Relation between the mean worst-case generalization gap and Rademacher complexity</strong></p> \[\mathbb{E}_{S\sim D^m} \sup_{h\in\mathcal{H}} L_D(h) - L_S(h) = 2 \mathbb{E}_{S\sim D^m} \mathrm{Rad}((\ell \circ \mathcal{H})\vert_S)\] <p>where \((\ell \circ \mathcal{H})\vert_S = \{(\ell(h,z_1), \dots, \ell(h,z_m)): h\in \mathcal{H}\}\)</p> <details><summary>Proof for the equation</summary> \[\begin{align*}\mathbb{E}_{S\sim D^m} \sup_{h\in\mathcal{H}} L_D(h) - L_S(h) &amp;=\mathbb{E}_{S\sim D^m} \sup_{h\in\mathcal{H}} [\mathbb{E}_{S'\sim D^m}[L_{S'}(h)] - L_S(h)]\\ &amp;\le \mathbb{E}_{S,S'\sim D^m} \sup_{h\in\mathcal{H}} [L_{S'}(h) - L_S(h)] \\ &amp;= \mathbb{E}_{S,S'\sim D^m} \sup_{h\in\mathcal{H}} \frac 1m \sum_{i=1}^m [\ell(h, z_i) - \ell(h, z_i')]\end{align*}\] <p>Let \(\sigma_i \in [-1,1]\) for \(i \in [m]\), \(P(\sigma_i=1)= P(\sigma_i=-1)= \frac12\), and \(\vec \sigma = (\sigma_1, \dots, \sigma_m)\). Let \((u_i, u_i')=\begin{cases}(z_i, z_i'), &amp;\text{if } \sigma_i=1 \\ (z_i', z_i), &amp;\text{if } \sigma_i=-1 \end{cases}\). So, for any value of \(S, S',\) and \(\vec \sigma\), defining \(U=(u_1, \dots, u_m)\) and \(U'=(u_1', \dots, u_m')\) accordingly, we have \(\ell(h, z_i) - \ell(h, z_i')=\sigma_i (\ell(h, u_i) - \ell(h, u_i'))\). Thus,</p> \[\begin{align*} \mathbb{E}_{S,S} \sup_{h\in\mathcal{H}} \frac 1m \sum_{i=1}^m [\ell(h, z_i) - \ell(h, z_i')] &amp;=\mathbb{E}_{S, S'} \mathbb{E}_{\vec \sigma} \mathbb{E}_{U,U'} [\sup_{h\in\mathcal{H}}\frac{1}{m} \sum_{i=1}^m \sigma_i (\ell(h, u_i) - \ell(h, u_i')) \vert S,S',\vec \sigma]\\ &amp;=\mathbb{E}_{U, U'} \mathbb{E}_{\vec \sigma} \mathbb{E}_{S,S'} [\sup_{h\in\mathcal{H}}\frac{1}{m} \sum_{i=1}^m \sigma_i (\ell(h, u_i) - \ell(h, u_i')) \vert U,U',\vec \sigma]\\ &amp;=\mathbb{E}_{U, U'} \mathbb{E}_{\vec \sigma}[\sup_{h\in\mathcal{H}}\frac{1}{m} \sum_{i=1}^m \sigma_i (\ell(h, u_i) - \ell(h, u_i'))]\\ &amp;=\mathbb{E}_{S, S'} \mathbb{E}_{\vec \sigma}[\sup_{h\in\mathcal{H}}\frac{1}{m} \sum_{i=1}^m \sigma_i (\ell(h, z_i) - \ell(h, z_i'))]\\ &amp;\le \mathbb{E}_{S} \mathbb{E}_{\vec \sigma} \sup_{h\in\mathcal{H}}\frac{1}{m} \sum_{i=1}^m \sigma_i \ell(h, z_i)+\mathbb{E}_{S'} \mathbb{E}_{\vec \sigma} \sup_{h\in\mathcal{H}}\frac{1}{m} \sum_{i=1}^m -\sigma_i \ell(h, z_i')\\ &amp;= 2 \mathbb{E}_{S} \mathbb{E}_{\vec \sigma} \sup_{h\in\mathcal{H}}\frac{1}{m} \sum_{i=1}^m \sigma_i \ell(h, z_i)\\ &amp;= 2 \mathbb{E}_{S\sim D^m} \mathrm{Rad}((\ell \circ \mathcal{H})|_S) \end{align*}\] </details> <p><br/></p> <p><strong>Compute \(\rm{Rad}((\ell \circ \mathcal{H})\vert_S)\)</strong></p> <p><strong>Talagrand‚Äôs contraction lemma.</strong> Let \(\phi : \mathbb{R}^m ‚Üí \mathbb{R}^m\) be given by \(\phi(t) = (œÜ_1(t_1), \dots, œÜ_m(t_m))\), where each \(œÜ_i\) is \(œÅ\)-Lipschitz (<a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuity</a>). Then</p> \[\mathrm{Rad}(\phi \circ V) = \mathrm{Rad}(\{\phi(v): v\in V\}) \le \rho \mathrm{Rad}(V)\] <p>Thus, if $\ell$ is \(œÅ\)-Lipschitz, we have</p> \[\rm{Rad}((\ell \circ \mathcal{H})\vert_S) \le \rho \rm{Rad}(\mathcal{H}\vert_S)\] <details><summary><strong>Example:</strong> logistic regression with bounded \(\mathcal{H}\)</summary> <ul> <li>Input space \(Z = X \times Y, X = \mathbb{R}^d, Y=\{-1,1\}\)</li> <li>Hypothesis class \(\mathcal{H}=\{x\to w\cdot x : w\in \mathbb{R}^d, \Vert w\Vert \le B\}\)</li> <li>Logistic loss \(\ell_{\rm{log}} (h,z) =l_y(h(x)) = \log(1+\exp(-y h(x)))\), which is 1-Lipschitz</li> </ul> \[\begin{align*} \mathrm{Rad} ((l_y\circ \mathcal{H})\vert_S) &amp;\le \mathrm{Rad} (\mathcal{H}|_S)\\ &amp;= \mathbb{E}_\sigma \sup_{\Vert w\Vert \le B} \frac{1}{m} \sum_{i=1}^m \sigma_i \left\langle w, x_i\right\rangle \\ &amp;= \frac{1}{m} \mathbb{E}_\sigma \sup_{\Vert w\Vert \le B} \left\langle w, \sum_{i=1}^m \sigma_i x_i\right\rangle \\ &amp;\le \frac{B}{m} \mathbb{E}_\sigma \Vert \sum_{i=1}^m \sigma_i x_i\Vert \\ &amp;\le \frac{B}{m} \sqrt{\mathbb{E}_\sigma \Vert \sum_{i=1}^m \sigma_i x_i\Vert^2} \\ &amp;= \frac{B}{m} \sqrt{\mathbb{E}_\sigma \sum_{i,j} \left\langle \sigma_i x_i, \sigma_j x_j \right\rangle }\\ &amp;= \frac{B}{m} \sqrt{\mathbb{E}_\sigma \sum_{i} \Vert x_i\Vert^2 + \sum_{i\neq j} \mathbb{E}_\sigma \sigma_i \sigma_j \left\langle x_i, x_j \right\rangle }\\ &amp;= \frac{B}{m} \sqrt{\sum_{i=1}^m \Vert x_i\Vert^2} \end{align*}\] \[\mathbb{E}_S \mathrm{Rad} ((l_y\circ \mathcal{H})\vert_S) \le \frac{B }{\sqrt{m}} \mathbb{E}_S \sqrt{\frac{1}{m} \sum_{i=1}^m \Vert x_i\Vert^2} \le \frac{B }{\sqrt{m}} \sqrt{\mathbb{E}_x \Vert x\Vert^2}\] </details> <p><br/></p> <details><summary><strong>Special case:</strong> 0-1 loss of binary classification</summary> <ul> <li>Input space \(Z = X \times Y, X = \mathbb{R}^d, Y=\{-1,1\}\)</li> <li>Hypothesis class \(\mathcal{H}=\{x\to h(x) : h(x)\in \{-1, 1\}\}\)</li> <li>0-1 loss \(\ell_{\rm{0-1}} (h,z) =l_y(h(x)) = \mathbb{1}_{h(x)\neq y}\) <ul> <li>0-1 loss is not a function on \(\mathbb{R}\), so applying Talagrand‚Äôs lemma is a little weird. For computing the loss, we can just extend the function \(l_y\) to \(\mathbb{R}\) in any way at all, and the loss will be exactly the same.</li> <li>So we could pick a \(\frac12-\)Lipschitz function. According to Talagrand‚Äôs lemma, we have \(\mathrm{Rad}(\ell_{0-1} \circ \mathcal{H}_{\pm1}) \le \frac12 \mathrm{Rad}(\mathcal{H}_{\pm1})\)</li> </ul> </li> </ul> <p>\(\mathcal{H}\vert_S \subseteq \{-1,1\}^m\) is finite, so we could bound the Rademacher complexity of this finite set based on its size. We have</p> \[\mathrm{Rad}(\mathcal{H}_{\pm1}\vert_S) \le \sqrt{\frac{2}{m} \log \vert\mathcal{H}_{\pm1}\vert_S\vert}\] <blockquote> <p>Growth function \eqref{eq:growth-function} provides the bound of \(\vert \mathcal{H}_{\pm1}\vert_S\vert\).</p> </blockquote> <p><em>Extension: for binary classifiers to {0, 1}, \(\mathrm{Rad}(\mathcal{H}_{0,1}\vert_S)=\mathrm{Rad}(\frac12 (\mathcal{H}_{\pm1} + 1)\vert_S)=\frac12\mathrm{Rad}(\mathcal{H}_{\pm1}\vert_S)\).</em></p> <p>We prove this by utilizing the following lemma and properties of sub-Gaussian distribution. <strong>Rademacher complexity of an arbitrary finite set.</strong> If \(V=\{v: v\in\mathbb{R}^m\}\) is finite, \(\Vert v\Vert \le B\) for all \(v \in V\), then</p> \[\mathrm{Rad} (V)\le \frac{B}{m} \sqrt{2\log \vert V\vert}\] <p><em>Proof.</em> We have \(\mathrm{Rad}(V) = \mathbb{E}_{\sigma\sim\rm{Unif}(\pm1)^m} \sup_{v\in V} \sum_{i=1}^m \frac{\sigma_i v_i}{m}\). According to [[Sub-Gaussian distribution#Sum]], \(\sigma_i \in SG(\frac{1-(-1)}{2})=SG(1)\) and \(\frac{\sigma_i v_i}{m} \in SG(|v_i|/m)\). As each \(v_i\sigma_i\) is independent of each other, we have \(\sum_{i=1}^m \frac{\sigma_i v_i}{m} \in SG(\sqrt{\sum_i v_i^2} /m)=SG(\frac{\Vert v\Vert }{m}) \subseteq SG(\frac{B}{m})\). According to <a href="https://en.wikipedia.org/wiki/Sub-Gaussian_distribution"> Sub-Gaussian distribution </a>, \(\sum_{i=1}^m \frac{\sigma_i v_i}{m}\) is zero-mean random variables that satisfy</p> \[\mathbb{E}_\sigma \left[ \sup_{v_i\in V} \sum_{i=1}^m \frac{\sigma_i v_i}{m} \right] \le \frac{B}{m} \sqrt{2 \log(\vert V\vert)}.\] <p>Thus, \(\mathrm{Rad}(\mathcal{H}_{\pm1}\vert_S) \le \sqrt{\frac{2}{m} \log \vert\mathcal{H}_{\pm1}\vert_S\vert}\)</p> </details> <p><br/></p> <p><strong>High probability bounds with Rademacher complexity</strong> ‚Äì McDiarmid‚Äôs inequality</p> <p>We need to convert the average-case bound into a high probability bound using a concentration inequality - <a href="https://en.wikipedia.org/wiki/McDiarmid%27s_inequality">McDiarmid‚Äôs inequality</a>, which is a generalized version of <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding‚Äôs inequality</a>. It could be used to bound the deviation between the sampled value and the¬†expected value¬†of certain functions (i.e., \(\sup_h [L_D(h)-L_S(h)]\)) when they are evaluated on¬†independent random variables.</p> <p><strong>McDiarmid‚Äôs inequality for the worst-case generalization gap.</strong> Assume loss is bounded that \(\ell(h,z) \in [a,b]\) for all \(h\) and \(z\). Let \(S=(z_1,\dots,z_i, \dots z_m)\) and \(S^{(i)}=(z_1, \dots, z_i', \dots, z_m)\). Then,</p> \[\begin{align*} \sup_{h\in\mathcal{H}} L_D(h)-L_S(h) &amp;= \sup_{h\in\mathcal{H}}L_D(h) - L_{S^{(i)}}(h) + L_{S^{(i)}}(h) - L_S(h) \\ &amp;\le \sup_{h\in\mathcal{H}} L_D(h) - L_{S^{(i)}}(h) + \sup_{h\in\mathcal{H}}L_{S^{(i)}}(h) - L_S(h) \\ &amp;\le \sup_{h\in\mathcal{H}} L_D(h) - L_{S^{(i)}}(h) + \frac{1}{m}(b-a) \end{align*}\] <p>We could substitute \(S\) and \(S^{(i)}\) without affecting the result, then the absolute value of difference is also bounded that \(\vert \sup_{h\in\mathcal{H}} [L_D(h)-L_S(h)] - \sup_{h\in\mathcal{H}} [L_D(h) - L_{S^{(i)}}(h)]\vert \le \frac{1}{m}(b-a)\). Given the bounded differences property, we could apply McDiarmid‚Äôs inequality directly. Thus, with probability at least \(1-\delta\), we have</p> \[\begin{align*}\sup_{h\in\mathcal{H}} L_D(h)-L_S(h) &amp;\le \mathbb{E}_{S\sim D^m} \sup_{h\in\mathcal{H}} L_D(h)-L_{S}(h) + (b-a)\sqrt{\frac{1}{2m}\log\frac{1}{\delta}}\\ &amp;\le 2 \mathbb{E}_{S\sim D^m} \mathrm{Rad}((\ell \circ\mathcal{H})\vert_S) + (b-a)\sqrt{\frac{1}{2m}\log\frac{1}{\delta}} \end{align*}\] <p><strong>The overall bound on estimation error</strong></p> \[\begin{align*} &amp;\mathrm{Pr}\left(\sup_{h\in\mathcal{H}} [L_D(h) - L_S(h)] &gt; 2 \mathbb{E}_{S\sim D^m} \mathrm{Rad}((\ell \circ\mathcal{H})\vert_S) + (b-a)\sqrt{\frac{1}{2m}\log\frac{1}{\delta}} \right) &lt; \delta \\ &amp;\mathrm{Pr}\left(L_S(h^*) - L_D(h^*) &gt; (b-a) \sqrt{\frac{1}{2m}\log\frac{1}{\delta}} \right) &lt; \delta \end{align*}\] <p>With probability at least \(1-\delta\), the estimation error satisfy</p> \[\begin{align*} L_D(\hat h_S) - L_D(h^*) &amp;\le \sup_{h\in\mathcal{H}} [L_D(h)-L_S(h)] + L_S(h^*) - L_D(h^*) \\ &amp;\le 2 \mathbb{E}_{S\sim D^m} \mathrm{Rad}((\ell \circ\mathcal{H})|_S) + (b-a)\sqrt{\frac{2}{m}\log\frac{2}{\delta}} \end{align*}\] <hr/> <h3 id="vc-dimension">VC dimension</h3> <ul> <li>VC dimension is a measure of the size of a class of sets. The notion can be extended to classes of <strong>binary</strong> functions.</li> <li>It is defined as the¬†cardinality¬†of the largest set of points that the algorithm can¬†shatter, which means the algorithm can always learn a perfect classifier for any labeling of at least one configuration of those data points.</li> </ul> <p>The binary classification case in Rademacher complexity section have shown that for hypothesis \(\mathcal{H}_{\pm1}=\{x\to h(x) : h(x)\in \{-1, 1\}\}\), we have \(\mathrm{Rad}(\mathcal{H}_{\pm1}\vert_S) \le \sqrt{\frac{2}{m} \log \vert\mathcal{H}_{\pm1}\vert_S\vert}\). If we use zero-one loss, then \(\mathrm{Rad}(\ell_{0-1} \circ \mathcal{H}_{\pm1}) \le \frac12 \mathrm{Rad}(\mathcal{H}_{\pm1})\). However, \(\mathrm{Rad}(\mathcal{H}_{\pm1}\vert_S)\) depends on particular distribution \(S\), which is not straightforward when taking expectation on it. So we need to upper bound it a little bit to make it look nicer <em>(not \(\vert \mathcal{H}\vert_S\vert \le \vert\mathcal{H}\vert\), that would be too loose!)</em></p> <p><strong>Growth function.</strong> The growth function \(Œì_\mathcal{H}(m)\) of a hypothesis class \(\mathcal{H}\) is given by</p> <p>\begin{equation}\label{eq:growth-function}\Gamma_\mathcal{H}(m)=\sup_{x_1, \dots, x_m\in \mathcal{X}}\vert \mathcal{H}\vert_{(x_1, \dots, x_m)}\vert \end{equation}</p> <p>By definition, \(\vert\mathcal{H}\vert_S\vert \le Œì_\mathcal{H}(m)\) for any \(S_x\) of size \(m\); thus for binary classifiers with zero-one loss, we immediately know that the expected worst-case generalization gap</p> \[\mathbb{E}_{S\sim D^m} \sup_{h\in\mathcal{H}} L_D(h)-L_{S}(h) \le 2 \mathbb{E}_{S\sim D^m} \mathrm{Rad}((\ell_{0,1} \circ \mathcal{H})|_S) \le \sqrt{\frac{2}{m} \log \Gamma_\mathcal{H}(m)}\] <p><strong>Shatter.</strong> A hypothesis class \(\mathcal{H}\) is said to shatter a set \(S_x ‚äÜ \mathcal{X}\) if it can achieves all possible labellings of \(S_x\), i.e. \(\vert \mathcal{H}\vert_{S_x}\vert = 2^m\).</p> <p><strong>VC dimension.</strong> The VC dimension of \(\mathcal{H}\) is the size of the largest set \(\mathcal{H}\) can shatter:</p> \[\mathrm{VCdim}(\mathcal{H})=\max\{m\ge0: \Gamma_\mathcal{H}(m) = 2^m\}\] <p>If \(\mathcal{H}\) can shatter unboundedly large sets, we say its VC dimension is infinite. We can bound the growth function in terms of the VC dimension: \(\Gamma_\mathcal{H}(m)= \mathcal{O}(m^{\mathrm{VCdim}(\mathcal{H})})\).</p> <p><strong>Compute VC dimension</strong></p> <ul> <li>\(\mathrm{VCdim}(\{x \to \mathbb{1}(x\ge a) :a‚àà\mathbb{R}\})=1\).</li> <li>\(\mathrm{VCdim}(\{x\to \mathrm{sgn}(w\cdot x) : w\in \mathbb{R}^d\}) = d\).</li> <li>\(\mathrm{VCdim}(\{x\to w\cdot x + b : w\in \mathbb{R}^d, b\in \mathbb{R} \}) = d+1\).</li> </ul> <p><strong>Bound the growth function</strong></p> <ul> <li>\(m \le \mathrm{VCdim}(\mathcal{H})\) case: \(\Gamma_\mathcal{H}(m) = 2^m\)</li> <li>\(m &gt; \mathrm{VCdim}(\mathcal{H})\) case: \(\Gamma_\mathcal{H}(m)\le (\frac{em}{\mathrm{VCdim}(\mathcal{H})})^{\mathrm{VCdim}(\mathcal{H})}\). In this case,</li> </ul> \[\mathbb{E}_{S\sim D^m} \sup_{h\in\mathcal{H}} L_D(h)-L_{S}(h) \le \sqrt{\frac{2\mathrm{VCdim}(\mathcal{H})}{m} (1+\log m-\log \mathrm{VCdim}(\mathcal{H}))}\] <p>When \(\mathrm{VCdim}(\mathcal{H}) ‚â• 3\), we can replace \(\log m + 1 ‚àí \log \mathrm{VCdim}(\mathcal{H})\) with simply \(\log m\) above.</p> <p><strong>The overall bound on estimation error</strong></p> <p>If \(m &gt; \mathrm{VCdim}(\mathcal{H})\) and we use zero-one loss</p> \[\begin{align*} &amp;\mathrm{Pr}\left(\sup_{h\in\mathcal{H}} [L_D(h) - L_S(h)] &gt; \sqrt{\frac{2\mathrm{VCdim}(\mathcal{H})}{m} (1+\log m-\log \mathrm{VCdim}(\mathcal{H}))} + \sqrt{\frac{1}{2m}\log\frac{1}{\delta}} \right) &lt; \delta \\ &amp;\mathrm{Pr}\left(L_S(h^*) - L_D(h^*) &gt; \sqrt{\frac{1}{2m}\log\frac{1}{\delta}} \right) &lt; \delta \end{align*}\] <p>With probability at least \(1-\delta\), the estimation error satisfy</p> \[\begin{align*} L_D(\hat h_S) - L_D(h^*) &amp;\le \sup_{h\in\mathcal{H}} [L_D(h)-L_S(h)] + L_S(h^*) - L_D(h^*) \\ &amp;\le \sqrt{\frac{2\mathrm{VCdim}(\mathcal{H})}{m} (1+\log m-\log \mathrm{VCdim}(\mathcal{H}))} + \sqrt{\frac{2}{m}\log\frac{2}{\delta}} \end{align*}\] <hr/> <h4 id="lower-bound">Lower bound</h4> <p>if we only know upper bounds, we never really know how tight they are, and so we can never really know if one algorithm is better than another.</p> <p><strong>No free lunch</strong></p> <p>Let \(\mathcal{H}\) be a hypothesis set of binary classifiers over \(\mathcal{X}\). Let \(m ‚â§ \mathrm{VCdim}(\mathcal{H})/2.\) Then</p> \[\inf_{\mathcal{A}}\sup_{D\ realizable\ by\ \mathcal{H}}\mathrm{Pr}_{S\sim D^m}\left(L_D(\mathcal{A}(S)) \ge \frac18 \right) \ge \frac17\] <p>where the infimum over \(\mathcal{A}\) is over all learning algorithms which return hypotheses in \(\mathcal{H}\). This theorem implies Any \(\mathcal{H}\) with \(\mathrm{VCdim}(\mathcal{H}) = ‚àû\) is not PAC learnable.</p> <ul> <li>a ‚Äúno free lunch‚Äù theorem, in that there is no algorithm that always works (in the sense of PAC learning): every algorithm fails on at least one distribution.</li> </ul> <p><strong>Lower bound based on VC dimension</strong></p> <p>Let \(\mathcal{H}\) be a hypothesis set of binary classifiers over \(\mathcal{X}\). For any \(m\ge 1\),</p> \[\inf_{\mathcal{A}}\sup_{D\ realizable\ by\ \mathcal{H}}\mathrm{Pr}_{S\sim D^m}\left(L_D(\mathcal{A}(S)) \ge \frac{\mathrm{VCdim}(\mathcal{H})-1}{32m} \right) \ge \frac{1}{100}\] <p>where \(L_D\) uses zero-one loss, and the infimum over \(\mathcal{A}\) is over all learning algorithms returning hypotheses in \(\mathcal{H}\).</p> <hr/> <h1 id="beyond-uniform-convergence">Beyond uniform convergence</h1> <p>if we don‚Äôt know what the optimal predictor looks like, we can just try a bunch of different H, which induces the concept of non-uniform bound and structural risk minimization</p> <h2 id="non-uniform-learning">Non-uniform learning</h2> <p>Let \(\mathcal{H}=\mathcal{H}_1\bigcup\mathcal{H}_2\bigcup \cdots\), a set of weights \(w_k ‚â• 0\) and \(\sum_{k}w_k \le 1\). Assume that each \(\mathcal{H}_k\) has uniform convergence:</p> \[\mathrm{Pr}_{S\sim D^m}(\sup_{h\in\mathcal{H}_k} \ L_S(h) - L_D(h) \le \varepsilon_k(m, \delta)) \ge 1-\delta\] <p>where for all \(k\) and \(\delta\in(0,1)\), \(\lim_{m\to \infty} \varepsilon_k(m, \delta) =0\).<br/> Then for any \(D\), with probability at least \(1 ‚àí Œ¥\) over the choice of \(S ‚àº D^m\) , we have</p> \[‚àÄh‚àà\mathcal{H}. \ L_D(h)‚â§L_S(h)+ \min_{k:h‚àà\mathcal{H}_k}\varepsilon_k(m, w_k \delta)\] <details><summary>Proof for non-uniform bound</summary> <p>Given a set of weights \(w_k ‚â• 0\) on \(\mathcal{H}_k\), for each \(k\) we have</p> \[\mathrm{Pr}_{S\sim D^m}(\exists h \in\mathcal{H}_k. \ L_S(h) - L_D(h) &gt; \varepsilon_k(m, w_k \delta)) &lt; w_k \delta\] <p>Then,</p> \[\begin{align*}\mathrm{Pr}_{S\sim D^m}\left( \bigcap_k (\forall h \in\mathcal{H}_k. \ L_S(h) - L_D(h)) \le \varepsilon_k(m, w_k \delta)\right) \ge 1 - \sum_k w_k \delta\\ \Rightarrow \mathrm{Pr}_{S\sim D^m}\left(‚àÄh‚àà\mathcal{H}. \ L_D(h)‚â§L_S(h)+ \min_{k:h‚àà\mathcal{H}_k}\varepsilon_k(m, w_k \delta) \right) \ge 1-\delta\end{align*}\] </details> <p><br/></p> <h3 id="structural-risk-minimization-srm">Structural risk minimization (SRM)</h3> <p>SRM is then the algorithm that minimizes this upper bound on \(L_D(h)\)</p> <p><strong>Definition 1.</strong> Let \(\mathcal{H}=\bigcup_{k:w_k&gt;0}\mathcal{H}_k\), a set of weights \(w_k ‚â• 0\) and \(\sum_{k}w_k \le 1\). Given the uniform convergence bound on the decomposition of \(\mathcal{H}\), structural risk minimization is given by:</p> \[\mathrm{SRM}_{\mathcal{H}}(S) \in \arg\min_{h\in\mathcal{H}} L_S(h)+ \varepsilon_{k_h}(m, w_{k_h} \delta) \ \text{ where }k_h\in\min_{k:h‚àà\mathcal{H}_k}Œµ_k(m,Œ¥w_k)\] <p>where \(k: h\in\mathcal{H}_k\) refers to the index of the hypothesis classes where \(h\) lives in (there is at least one \(\mathcal{H}_k\) that contains \(h\)).</p> <p><strong>Definition 2.</strong> Let \(\mathcal{H}=\bigcup_{k:w_k&gt;0}\mathcal{H}_k\), a set of weights \(w_k ‚â• 0\) and \(\sum_{k}w_k \le 1\). If we could eliminate the dependence on \(\delta\) of \(\varepsilon\), the structural risk minimization can also be defined by:</p> \[\mathrm{SRM}_{\mathcal{H}}(S) \in \arg\min_{h\in\mathcal{H}} L_S(h)+ \varepsilon_{k_h}(m, w_{k_h}) \ \text{ where }k_h\in\min_{k:h‚àà\mathcal{H}_k}Œµ_k(m,w_k)\] <p>This bound don‚Äôt need to commit to a certain \(\delta\) during training, so it is better to use in pratice.</p> <p><strong>Estimation error bound of SRM</strong></p> <p>Let \(\hat h_S=\mathrm{SRM}_{\mathcal{H}}(S)\) for simplification and \(h^*\) be any fixed function in \(\mathcal{H}\), we have</p> \[\begin{align*} \mathrm{Pr}\left(L_D(\hat h_S) &gt; L_S(h^*) + \varepsilon_{k_{h^*}}(m, \frac{1}{2} w_{k_{h^*}} \delta) \right) &lt; \mathrm{Pr}\left(L_D(\hat h_S) &gt; L_S(\hat h_S) + \varepsilon_{k_{\hat h_S}}(m, \frac{1}{2} w_{k_{\hat h_S}} \delta) \right) &lt; \frac12\delta \\ \mathrm{Pr}\left(L_S(h^*) - L_D(h^*) &gt; (b-a) \sqrt{\frac{1}{2m}\log\frac{2}{\delta}} \right) &lt; \frac12\delta \end{align*}\] <p>Then, with probability at least \(1-\delta\) over the choice of \(S \sim D^m\), we have</p> \[\begin{align*} L_D(\mathrm{SRM}_{\mathcal{H}}(S)) - L_D(h^*) &amp;\le \varepsilon_{k_{h^*}}(m, \frac{1}{2} w_{k_{h^*}} \delta) + (b-a) \sqrt{\frac{1}{2m}\log\frac{2}{\delta}} \end{align*}\] <details><summary>Example: specific bound using Rademacher complexity</summary> <p>Here we introduce a specific bound using Rademacher complexity. Let the Rademacher complexity of \(k\)th hypothesis \(R_k = \mathbb{E}_{S\sim D^m} \mathrm{Rad} (\mathcal{H}_k\vert_S)\). According to the high probability bounds with Rademacher complexity, for each \(k\), we have</p> \[\begin{align*} \mathrm{Pr}_{S\sim D^m}\left(\sup_{h\in\mathcal{H}_k} \ L_S(h) - L_D(h) \le 2R_k + (b-a)\sqrt{\frac{1}{2m}\log\frac{1}{\delta}} \right) \ge 1-\delta \end{align*}\] <p>Then, let \(k_h=\arg\min_{k:h\in\mathcal{H}_k} 2R_{k_h} + (b-a)\sqrt{\frac{1}{2m}\log\frac{1}{w_{k_h}\delta}}\), we have</p> \[\begin{align*} \mathrm{Pr}_{S\sim D^m}\left(\sup_{h\in\mathcal{H}} \ L_S(h) - L_D(h) \le 2R_{k_h} + (b-a)\sqrt{\frac{1}{2m}\log\frac{1}{w_{k_h}\delta}} \right) \ge 1-\delta \end{align*}\] <p>To separate \(w_k\) and \(\delta\), we can choose \(w_k=\frac{6}{\pi^2k^2}\) such that with probability at least \(1-\delta\)</p> \[\begin{align*} \sup_{h\in\mathcal{H}} \ L_S(h) - L_D(h) &amp;\le 2R_{k_h} + (b-a)\sqrt{\frac{1}{2m}\log\frac{\pi^2{k_h}^2}{6\delta}} \\ &amp;\le 2R_{k_h} + (b-a)\sqrt{\frac{1}{2m}(2\log{k_h} + \log\frac{\pi^2}{6} + \log\frac{1}{\delta})}\\ &amp;\le 2R_{k_h} + (b-a)\sqrt{\frac{1}{2m}(2\log{k_h} + \log\frac{\sqrt{e}}{\delta})}\\ &amp;\le 2R_{k_h} + (b-a)\sqrt{\frac{1}{m}\log{k_h}} + (b-a)\sqrt{\frac{1}{2m}\log\frac{\sqrt{e}}{\delta}} \end{align*}\] <p>Then the SRM solution is given by\(\mathrm{SRM}_{\mathcal{H}}(S) \in \arg\min_{h\in\mathcal{H}} L_S(h)+ 2R_{k_h} + (b-a)\sqrt{\frac{1}{m}\log{k_h}}\)</p> <p>Let \(\hat h_S=\mathrm{SRM}_{\mathcal{H}}(S)\) , then with probability at least \(1-\delta\)</p> \[\begin{align*} L_D(\hat h_S) &amp;\le L_S(\hat h_S) + 2R_{k_{\hat h_S}} + (b-a)\sqrt{\frac{1}{m}\log{k_{\hat h_S}}} + (b-a)\sqrt{\frac{1}{2m}\log\frac{\sqrt{e}}{\delta}}\\ &amp;\le L_S(h^*) + 2R_{k_{h^*}} + (b-a)\sqrt{\frac{1}{m}\log{k_{h^*}}} + (b-a)\sqrt{\frac{1}{2m}\log\frac{\sqrt{e}}{\delta}}\\ &amp;\le L_D(h^*) + 2R_{k_{h^*}} + (b-a)\sqrt{\frac{1}{m}\log{k_{h^*}}} + (b-a)\sqrt{\frac{1}{2m}\log\frac{2\sqrt{e}}{\delta}}+(b-a) \sqrt{\frac{1}{2m}\log\frac{2}{\delta}} \\ &amp;\le L_D(h^*) + 2\mathbb{E}_{S\sim D^m} \mathrm{Rad} (\mathcal{H}_{k_{h^*}}|_S) + (b-a)\sqrt{\frac{1}{m}\log{k_{h^*}}} + (b-a)\sqrt{\frac{2}{m}\log\frac{3}{\delta}}) \end{align*}\] <p>where \(h^*\) is any function in hypothesis \(\mathcal{H}\).</p> </details> <p><br/></p> <hr/> <h2 id="algorithmic-stability">Algorithmic stability</h2> <ul> <li>Sometimes \(\mathcal{H}\) is too big for uniform convergence, but \(\mathcal{A}\) still learns well. Stability based bound is algorithm-specific that it can be insightful for models prone to non-uniform behavior, like deep networks.</li> <li>Stability-based bounds measure how much the model‚Äôs performance changes when the training data is slightly perturbed.</li> </ul> <p><strong>Randomness of algorithm</strong></p> <ul> <li>When we‚Äôre dealing with specific algorithms, though, it‚Äôs important to note that these algorithms might be randomized: for instance, stochastic gradient descent sees points in a random order, and might start at a random location.</li> <li>We‚Äôll always assume that this randomness is independent of \(S\) (other than its size): for instance, this will be the random seed that determines that pattern in which we access the \(z_i\) . If \(\mathcal{A}\) is deterministic, it‚Äôs just a point-mass distribution.</li> </ul> <p><strong>Notation.</strong> for changing single sample points in set \(S\) : If \(S = (z_1, \dots, z_{i-1}, z_i, z_{i+1}, \ldots, z_m)\), then \(S^{(i\leftarrow z')}=(z_1, \dots, z_{i-1}, z_i', z_{i+1}, \ldots, z_m)\).</p> <p><strong>Proposition.</strong> For any distribution \(\mathcal{D}\) and learning algorithm \(\mathcal{A}\),</p> \[\mathbb{E}_{S\sim D^m, \mathcal{A}} [L_D(\mathcal{A}(S)) - L_S(\mathcal{A}(S))] = \mathbb{E}_{\substack{S\sim D^m, z_i'\sim D\\ \mathcal{A}, i\sim\text{Unif}([m])}} [\ell(\mathcal{A}(S^{(i\leftarrow z')}), z_i) - \ell(\mathcal{A}(S), z_i)]\] <p><strong>Definition (average stability).</strong> \(\mathcal{A}\) is \(Œµ(m)\)-on-average-replace-one stable if for all \(D\),</p> \[\mathbb{E}_{\substack{S\sim D^m, z_i'\sim D\\ \mathcal{A}, i\sim\text{Unif}([m])}} [\ell(\mathcal{A}(S^{(i\leftarrow z')}), z_i) - \ell(\mathcal{A}(S), z_i)] \le \varepsilon (m)\] <p>Thus, an \(Œµ(m)\)-on-average-replace-one stable algorithm will have small average-case generalization gap \(\mathbb{E}_{S\sim D^m, \mathcal{A}} [L_D(\mathcal{A}(S)) - L_S(\mathcal{A}(S))]\).</p> <p><strong>Definition (uniform stability).</strong> \(\mathcal{A}\) is \(Œ≤(m)\)-uniformly stable if for all \(m ‚â• 1\) and all \(i\in\{1,\dots,m\}\)</p> \[\sup_{\substack{S\sim D^m \\ z, z'\sim D}} [\mathbb{E}_{\mathcal{A}} \ell(\mathcal{A}(S^{(i\leftarrow z')}), z) - \mathbb{E}_{\mathcal{A}}\ell(\mathcal{A}(S), z)] \le \beta (m)\] <p>That is, changing one point in any training set gives you a hypothesis that looks almost the same for any test point.</p> <p><strong>Upper bound \(\mathbb{E}_{\mathcal{A}} [L_D(\mathcal{A} (S))- L_S(\mathcal{A} (S))]\)</strong></p> <p><strong>Theorem (uniform-stability).</strong> Suppose that \(\ell(h, z) ‚àà [a, b]\) almost surely. Let \(\mathcal{A}\) be \(Œ≤(m)\)-uniformly stable. Then, with probability at least \(1 ‚àí Œ¥\) over the choice of training points \(S ‚àº D^m\),</p> \[\mathbb{E}_{\mathcal{A}} [L_D(\mathcal{A} (S))- L_S(\mathcal{A} (S))] \le \beta(m)+(2m\beta(m)+b-a) \sqrt{\frac{1}{2m} \log\frac{1}{\delta}}\] <p>The best case for this bound is when \(Œ≤(m) = \mathcal{O}(1/m)\), in which case you get a \(\mathcal{O}(1/\sqrt{m})\) rate.</p> <details><summary>Proof for the theorem</summary> <p>Let \(f(S)=\mathbb{E}_{\mathcal{A}} [L_D(\mathcal{A} (S))- L_S(\mathcal{A} (S))]\). If \(\mathcal{A}\) is \(\beta(m)\)-uniformly stable, then \(\mathbb{E}_S f(S)\le \beta(m)\). Assume \(\ell \in [a,b]\), let \(\hat h = \mathcal{A}(S), \hat h^i=\mathcal{A}(S^{(i \leftarrow z')})\).</p> <p>We have</p> \[\begin{align*} \vert \mathbb{E}_{\mathcal{A}} [L_D(\hat h^i)- L_D(\hat h)] \vert &amp;= \vert \mathbb{E}_{\mathcal{A}, z\sim D} \ell(\hat h^i, z) - \mathbb{E}_{\mathcal{A}}\ell(\hat h, z) \vert \\ &amp;\le \mathbb{E}_{z\sim D}\vert \mathbb{E}_{\mathcal{A}} \ell(\hat h^i, z) - \mathbb{E}_{\mathcal{A}}\ell(\hat h, z) \vert \\ &amp;\le \beta(m)\\ \end{align*}\] <p>Also, we have</p> \[\begin{align*} \vert\mathbb{E}_{\mathcal{A}} [L_S(\hat h)- L_{S^{(i\leftarrow z')}}(\hat h^i)]\vert &amp;= \frac{1}{m} \left\vert \mathbb{E}_{\mathcal{A}}\sum_{j=1}^m (\ell(\hat h, z_j) - \ell(\hat h^i, z_j))\right\vert\\ &amp;\le \frac{1}{m} \sum_{j\neq i} \left\vert \mathbb{E}_{\mathcal{A}} \ell(\hat h, z_j) - \ell(\hat h^i, z_j) \right\vert + \frac{1}{m} \left\vert \mathbb{E}_{\mathcal{A}} \ell(\hat h, z_i) - \ell(\hat h^i, z_i) \right\vert \\ &amp;\le \frac{m-1}{m} \beta(m) + \frac{b-a}{m} \\ &amp;\le \beta(m) + \frac{b-a}{m} \end{align*}\] <p>Combining the above two inequalities, we can show the bounded difference of \(f(S)\) that</p> \[\begin{align*}\vert f(S) - f(S^{(i \leftarrow z')})\vert &amp;= \left\vert\mathbb{E}_{\mathcal{A}} [L_D(\hat h)- L_S(\hat h) - L_D(\hat h^i) +L_{S^{(i \leftarrow z')}}(\hat h^i)] \right\vert \\ &amp;\le \left\vert\mathbb{E}_{\mathcal{A}} [L_D(\hat h)- L_D(\hat h^i)] \right\vert + \left\vert \mathbb{E}_{\mathcal{A}}[ L_S(\hat h) - L_{S^{(i \leftarrow z')}}(\hat h^i)]\right\vert \\ &amp;\le 2\beta(m) + \frac{b-a}{m} \end{align*}\] <p>According to <a href="https://en.wikipedia.org/wiki/McDiarmid%27s_inequality">McDiarmid‚Äôs inequality</a>, with probability at least \(1-\delta\)</p> \[\begin{align*} \mathbb{E}_{\mathcal{A}} [L_D(\mathcal{A} (S))- L_S(\mathcal{A} (S))] &amp;\le \mathbb{E}_{S} \mathbb{E}_{\mathcal{A}} [L_D(\mathcal{A} (S))- L_S(\mathcal{A} (S))] + \sqrt{\frac12 m(2\beta(m) + \frac{b-a}{m})^2 \log \frac{1}{\delta}}\\ &amp;\le \beta(m) + (2m\beta(m) + b-a)\sqrt{\frac{1}{2m} \log \frac{1}{\delta}} \end{align*}\] </details> <p><br/></p> <h3 id="regularized-loss-minimization">Regularized loss minimization</h3> <p>Major challenges for using ERM to explain deep learning:</p> <ul> <li><strong>Computational Hurdle:</strong> ERM minimizes the empirical loss over the training data. However, for complex deep learning models with millions of parameters, this optimization problem becomes computationally NP-hard.</li> <li><strong>Intractability of Uniform Convergence:</strong> Uniform convergence ERM bounds might not be enough for generalization. In practice, deep learning models often exhibit non-uniform convergence, where their performance can vary significantly across different data points.</li> <li><strong>Regularized Loss Minimization (RLM):</strong> In reality, we often observe that regularized loss minimization (RLM), which optimizes a trade-off between model complexity and loss, leads to better generalization performance even if it doesn‚Äôt strictly minimize the empirical risk.</li> </ul> <p>Therefore, while ERM remains a theoretical cornerstone for understanding generalization, regularized loss minimization emerges as the preferred approach in practice. By incorporating regularization terms that penalize model complexity, we can achieve better generalization performance and obtain more interpretable models.</p> <p>Regularized loss minimization (RLM) adds a regularization term (or regularizer)¬†to a¬†loss function:</p> \[\arg\min_{h\in \mathcal{H}} L_S(h) + ŒªR(h)\] <p>It is closely connected to constrained ERM:</p> \[\arg\min_{h\in \mathcal{H}: R(h) \le B} L_S(h)\] <p>they are Lagrange dual to each other: for any Œª, there is some B such that the solutions agree, and vice versa.</p> <p>But RLM is typically easier computationally, and it‚Äôs usually easier to choose a good Œª than to choose a good B.</p> <p>Suppose the loss function \(h ‚Üí \ell(h, z)\) is convex for each \(z\), and regularizer \(R(h)\) is 1-strongly convex. Then \(f_S(h) = L_S(h) + ŒªR(h)\), the sum of a convex function and a Œª-strongly convex function, is Œª-strongly convex. Let \(\mathcal{A}(S)\) denote \(\arg\min_{h\in \mathcal{H}} f_S(h)\); since \(f_S\) is strongly convex, it has a unique minimizer (so we can leave out the \(\mathbb{E}_\mathcal{A}\) term).</p> <p><strong>Upper bound \(L_D (\mathcal{A}(S))-L_D(h^*)\)</strong></p> <p>Assume the regularizer is \(R(h)\) is nonnegative, and \(R(h^*) \le \frac12 B^2\) for any \(h^*\). If \(L_S(h)\) is convex, \(R(h)\) is 1-strongly convex and \(l(h,z)\) is \(\rho\)-Lipschitz, we have</p> \[\mathbb{E}_S L_D(\mathcal{A}(S)) \le L_D(h^*)+ \frac12 \lambda B^2+ \frac{4\rho^2}{\lambda m}\] <p>For any fixed \(h^*\), it holds with probability at least \(1 ‚àí \delta\) that</p> \[L_D (\mathcal{A}(S)) \le L_D(h^*) + \frac \lambda 2 B^2 + \frac{4\rho^2}{\lambda m} + \frac{4\rho^2}{\lambda m} \sqrt{\frac m 2 \log \frac1 \delta}\] <p><code class="language-plaintext highlighter-rouge">Proof sketch.</code></p> <ul> <li>Given the convex and Lipschitz assumption, we can show RLM is \(\frac{4\rho^2}{\lambda m}\)-uniformly stable</li> <li>then bound \(\mathbb{E}_S L_D(\mathcal{A}(S)) - L_S(\mathcal{A}(S))\) using uniformly stability</li> <li>also, \(L_S(\mathcal{A}(S)) \le L_S(h^*)+R(h)\)</li> <li>Then \(\mathbb{E}_S L_D(\mathcal{A}(S)) \le L_D(h^*)+ \frac12 \lambda B^2+ \frac{4\rho^2}{\lambda m}\)</li> <li>Use McDiarmid‚Äôs inequality to derive the high probability bound\(L_D (\mathcal{A}(S)) \le \mathbb{E}_S L_D(\mathcal{A}(S)) + \frac{4\rho^2}{\lambda m} \sqrt{\frac m 2 \log \frac1 \delta}\)</li> </ul> <details><summary>Full proof</summary> <p>We first show that the function \(f_S(h)=L_S(h) + ŒªR(h)\) is \(\frac{4\rho^2}{\lambda m}\)-uniformly stable if \(L_S(h)\) is convex, \(R(h)\) is 1-strongly convex and \(l(h,z)\) is \(\rho\)-Lipschitz. Let \(S'=S^{(i \leftarrow z')}\), then</p> \[\begin{align*} f_S(h)-f_S(g)&amp;=f_S(h)-f_{S'}(h)+f_{S'}(h) - f_{S'}(g)+f_{S'}(g)-f_{S}(g)\\ &amp;=f_{S'}(h) - f_{S'}(g)+ \frac{1}{m} (\ell (h,z_i) - \ell(h, z_i')) + \frac{1}{m} (\ell (g,z_i') - \ell(g, z_i))\\ &amp;=f_{S'}(h) - f_{S'}(g)+ \frac{1}{m} (\ell (h,z_i) - \ell(g, z_i)) + \frac{1}{m} (\ell (g,z_i') - \ell(h, z_i')) \end{align*}\] <p>Let \(\hat h = \arg\min_h f_S(h)\) and \(\hat h^i = \arg\min_h f_S'(h)\). Then,</p> \[\begin{align*} f_S(\hat h^i) - f_S(\hat h) &amp;\le \frac{1}{m} (\ell (\hat h^i,z_i) - \ell(\hat h, z_i)) + \frac{1}{m} (\ell (\hat h,z_i') - \ell(\hat h^i, z_i')) \end{align*}\] <p>As \(f_S(h)\) is \(\lambda\)-strongly convex and \(\nabla f_S(\hat h)=0\), then</p> \[f_S(\hat h^i) - f_S(\hat h) \ge \langle \nabla f_S(\hat h), \hat h^i-\hat h \rangle +\frac12 \lambda \Vert \hat h^i-\hat h\Vert^2 = \frac12 \lambda \Vert\hat h^i-\hat h\Vert^2\] <p>Thus,</p> \[\begin{align*} \frac12 \lambda \Vert \hat h^i-\hat h\Vert ^2 &amp;\le \frac{1}{m} (\ell (\hat h^i,z_i) - \ell(\hat h, z_i)) + \frac{1}{m} (\ell (\hat h,z_i') - \ell(\hat h^i, z_i'))\le \frac{2\rho}{m} \Vert \hat h^i-\hat h\Vert \\ &amp;\Rightarrow \Vert \hat h^i-\hat h\Vert \le \frac{4\rho}{\lambda m} \\ &amp;\Rightarrow \vert\ell (\hat h,z_i') - \ell(\hat h^i, z_i')\vert \le \frac{4\rho^2}{\lambda m} \end{align*}\] <p>\(\mathcal{A}\) is \(\frac{4\rho^2}{\lambda m}\)-uniformly stable.</p> <p>Assume \(R(h)\) is non-negative, then</p> \[L_S(\mathcal{A}(S)) \le L_S(\mathcal{A}(S))+\lambda R(\mathcal{A}(S)) \le L_S(h^*)+\lambda R(h^*)\] <p>for all \(h^* \in \mathcal{H}\). Then \(\mathbb{E}_S L_S(\mathcal{A}(S)) \le L_D(h^*)+\lambda R(h^*)\).</p> \[\mathbb{E}_S L_D (\mathcal{A}(S)) \le \mathbb{E}_S L_S(\mathcal{A}(S)) + \frac{4\rho^2}{\lambda m}\le L_D(h^*) + \frac \lambda 2 B^2 + \frac{4\rho^2}{\lambda m}\] <p>where we don‚Äôt actually need to apply uniform-stability theorem directly.</p> </details> <p><br/></p>]]></content><author><name></name></author><category term="basics"/><category term="learning-theory"/><category term="note"/><summary type="html"><![CDATA[a short overview on techniques to prove learning bounds]]></summary></entry><entry><title type="html">Learning with biased labels</title><link href="https://he-zh.github.io/blog/2023/biased-label/" rel="alternate" type="text/html" title="Learning with biased labels"/><published>2023-02-01T00:00:00+00:00</published><updated>2023-02-01T00:00:00+00:00</updated><id>https://he-zh.github.io/blog/2023/biased-label</id><content type="html" xml:base="https://he-zh.github.io/blog/2023/biased-label/"><![CDATA[<p>Deep Neural Networks (DNNs) have gained popularity in a wide range of applications. The remarkable success of DNNs often relies on the availability of high-quality datasets. However, the acquisition of a large amount of well-annotated unambiguous data could be very expensive and sometimes even inaccessibl. Standard training using ambiguous data may produce overly confident models and thus leading to poor generalization.</p> <p>During my stay at Baidu Research, I have been have been actively engaged in the realm of learning with biased labels, with a specific focus on enhancing model robustness and reliability in the presence of noisy data. This presentation serves as a comprehensive overview of my work. It encompasses the following key components:</p> <ul> <li>A concise review of various methodologies for learning with noisy labels;</li> <li>A specific topic we brought up ‚Äì addressing the challenge of training data with highly ambiguous labels, such as those that provide an incomplete description of the object. (This part is temporarily deleted from this post.)</li> </ul> <p>I believe that our research may shed some light on how to build a more trustworthy machine learning model, especially in domains where data quality is a critical factor.</p> <object data="/assets/pdf/learning_with_biased_labels_hez.pdf" width="100%" height="500px"> <p>Unable to display PDF file. <a href="/assets/pdf/learning_with_biased_labels_hez.pdf">Download</a> instead.</p> </object>]]></content><author><name></name></author><category term="research"/><category term="survey"/><category term="paper-digest"/><category term="slides"/><category term="label-noise"/><summary type="html"><![CDATA[a review on robust learning with noisy labels]]></summary></entry><entry><title type="html">Sparse double descent where network pruning aggravates overfitting</title><link href="https://he-zh.github.io/blog/2022/sparsedd/" rel="alternate" type="text/html" title="Sparse double descent where network pruning aggravates overfitting"/><published>2022-07-12T00:00:00+00:00</published><updated>2022-07-12T00:00:00+00:00</updated><id>https://he-zh.github.io/blog/2022/sparsedd</id><content type="html" xml:base="https://he-zh.github.io/blog/2022/sparsedd/"><![CDATA[<p>Here I share our new work on network pruning ‚ÄúSparse Double Descent: Where Network Pruning Aggravates Overfitting‚Äù. This work was mainly inspired by recent studies on model over-parameterization and lottery tickets hypothesis, where we explored and analyzed the generalization performance of sparse neural networks. Main conclusion: The double descent phenomenon exists in sparse neural networks-as sparsity increases, the test accuracy of the model will first decrease, then increase, and finally decrease again.</p> <h2 id="motivation">Motivation</h2> <p>Machine learning models are widely believed to have difficulty minimizing both bias and variance at once. Thus finding the most appropriate model requires balancing these two factors. Here shows the traditional bias-variance tradeoff curve: as model capacity increases, the training error decreases, while the test error first decreases and then increases.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/u-curve-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/u-curve-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/u-curve-1400.webp"/> <img src="/assets/img/blog/sparsedd/u-curve.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Bias-variance tradeoff. </div> <p>However, in deep learning practice, large models often perform better than smaller models, despite traditional belief that too many parameters leads to overfitting. Studies have found that the relationship between test error and model capacity is not a U-shaped tradeoff, but rather a double descent curve, that is, as the number of model parameters increases, the test error first decreases, then increases, and then decreases again <d-cite key="belkin2019reconciling"></d-cite> <d-cite key="NakkiranKBYBS20"></d-cite>.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/doubledescent-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/doubledescent-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/doubledescent-1400.webp"/> <img src="/assets/img/blog/sparsedd/doubledescent.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Double descenr curve <d-cite key="NakkiranKBYBS20"></d-cite>. </div> <p>That is to say, an over-parameterized neural network, instead of severely overfitting, may have better generalization performance! This contradicts the traditional belief directly.</p> <p>The lottery tickets hypothesis <d-cite key="FrankleC19"></d-cite> provides a new way for explaining this phenomenon. According to the lottery ticket hypothesis, a randomly initialized dense network contains a well-performing sub-network that can achieve comparable accuracy to the original dense network when trained from the original initialization (winning ticket). More parameters in a network mean a higher chance of containing a sub-network with good performance, and thus a higher chance of winning the lottery.</p> <p>From this point of view, in an over-parameterized neural network, only a relatively small number of parameters play a role in optimization and generalization, while the remaining parameters only serve as redundant backups. The performance of models won‚Äôt be greatly affected when redundant parameters were pruned.</p> <p>It appears that we can safely prune redundant parameters from our models without worrying about adverse effects. Moreover, pruned neural networks are believed to have better generalization properties according to Occam‚Äôs razor principle <d-cite key="HoeflerABDP21"></d-cite>. The current pruning literature also emphasizes that their algorithm can maintain an accuracy comparable to the original model even when a significant number of parameters are pruned.</p> <p>In light of the double descent phenomenon, we wonder: Are the parameters removed by pruning completely redundant?</p> <p>We investigate this question following the deep double descent setting <d-cite key="NakkiranKBYBS20"></d-cite> and conduct extensive experiments on sparse neural networks.</p> <h2 id="sparse-double-descent">Sparse double descent</h2> <p>Experiments revealed that the ‚Äúredundant‚Äù parameters in the network are not completely redundant. When increasing model sparsity through iterative pruning, even if the model training accuracy has not been affected, its test accuracy may decline significantly, where the model overfits noise. If the sparsity of the model is further increased, it can be found that after passing the <a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent">interpolation threshold</a>, the training accuracy of the model begins to drop rapidly, and the test accuracy begins to increase, and the robustness of the model to noise is gradually improved. If you continue to reduce the parameters of the model after the test accuracy rate reaches its highest point, the training and testing accuracy of the model decreases at the same time, where the model gradually loses its learning ability. ‚Äã</p> <div class="row justify-content-center mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/dataset-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/dataset-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/dataset-1400.webp"/> <img src="/assets/img/blog/sparsedd/dataset.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sparse double descent on different datasets. Left: CIAFR-10. Middle: CIFAR-100. Right: Tiny ImageNet. </div> <p>In addition, we also found that using different criteria for pruning, the resulting models have different model capacity/complexity even with the same amount of parameters. For example, for the interpolating threshold, the model pruned using magnitude-based method has higher sparsity, while the model pruned with random pruning corresponds to lower sparsity. It shows that random pruning damages the representative capability of the model more severely, and fewer parameters can be pruned to achieve the same effect.</p> <div class="row justify-content-center mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/pruning-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/pruning-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/pruning-1400.webp"/> <img src="/assets/img/blog/sparsedd/pruning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sparse double descent with different pruning methods. Left: magnitude based pruning. Middle: gradient based pruning. Right: random pruning. </div> <p>While most of our experiments retrained the lottery ticket hypothesis, several other different approaches were also applied. Interestingly, a significant double drop can be observed even with finetuning after pruning. It can be seen that the phenomenon of sparse double descent is not limited to training a sparse network from initialization. ‚Äã</p> <div class="row justify-content-center mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/retrain-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/retrain-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/retrain-1400.webp"/> <img src="/assets/img/blog/sparsedd/retrain.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sparse double descent with different retraining methods. Left: finetuning. Middle: learning rate rewinding. Right: scratch retraining. </div> <p>We also adjusted the label noise ratio in our experiments. Similar to the deep double descent, increasing the label noise ratio will make the starting point of the model training accuracy drop to move towards a higher model capacity (ie, lower sparsity). On the other hand, the higher label noise ratio, the more parameters need to be pruned to avoid overfitting. ‚Äã</p> <div class="row justify-content-center mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/noise-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/noise-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/noise-1400.webp"/> <img src="/assets/img/blog/sparsedd/noise.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sparse double descent under different label noise ratio. Left: 20%. Middle: 40%. Right: 80%. </div> <h2 id="why-sparse-double-descent-happens">Why sparse double descent happensÔºü</h2> <p>Here we mainly investigate two possible explanations.</p> <p>One is the Minima Flatness Hypothesis. Some papers point out that pruning can add perturbations to the model, which make it easier for the model to converge to a flat minimum <d-cite key="BartoldsonMBE20"></d-cite>. Since flatter minima generally have better generalization ability, so <d-cite key="BartoldsonMBE20"></d-cite> believes that pruning affects the generalization of the model by affecting the minima flatness.</p> <p>So, can minima flatness explain the sparse double descent?</p> <p>We visualized the loss as shown in the figure, and indirectly compared the flatness of the minima of the model under different sparsity.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/sharpness-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/sharpness-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/sharpness-1400.webp"/> <img src="/assets/img/blog/sparsedd/sharpness.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Loss visualization. </div> <p>Unfortunately, as sparsity increases, the loss curve becomes sharper. There is no evident correlation between the minima flatness and the test accuracy.</p> <p>The other is the Learning Distance Hypothesis.</p> <p>It has been proved theoretically that the complexity of deep learning models is closely related to the l2 distance of the parameters from the initialization (learning distance) <d-cite key="Nagarajan19"></d-cite>. When the model is obtained through early stopping, the model is close to the initialization and there is not enough complexity to memorize noise. And the overly trained model has higher complexity and is easy to overfit.</p> <p>So, may changes in learning distance reflect the trend of double decline? ‚Äã</p> <div class="row justify-content-center mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/distance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/distance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/distance-1400.webp"/> <img src="/assets/img/blog/sparsedd/distance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The curve of learning distance and test accuracy. </div> <p>As can be seen from the figure, when the accuracy rate decreases, the overall learning distance tends to increase, and the highest point corresponds to the lowest point of the accuracy rate; when the accuracy rate increases, the learning distance also decreases accordingly. The change in learning distance is generally in line with the trend of sparse double descent (although when the test accuracy declines for the second time, it is difficult for the learning distance to rise again due to too few trainable parameters).</p> <h2 id="relation-to-the-lottery-ticket-hypothesis">Relation to the lottery ticket hypothesis</h2> <p>We also conduct experiments comparing winning tickets with re-random initialization. Interestingly, the initialization of the lottery ticket hypothesis does not always outperform the re-initialization of the network in the double-descent scenario. ‚Äã</p> <div class="row justify-content-center mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/sparsedd/reinit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/sparsedd/reinit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/sparsedd/reinit-1400.webp"/> <img src="/assets/img/blog/sparsedd/reinit.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Train and test accuracy of lottery ticket initialization and random reinitialization. </div> <p>It can be seen from the figure that the result of Reinit is shifted to the left compared to Lottery as a whole, that is to say, the Reinit method is inferior to Lottery in terms of retaining the expressive ability of the model. This also validates the lottery ticket hypothesis: even if the structure of the model is the same, the performance of the model may be very different when trained from different initializations.</p> <h2 id="conclusion">Conclusion</h2> <p>In the process of doing this research, we observed some amazing and counterintuitive experimental phenomena and attempted to interpret them analytically. However, the existing theoretical work has not been able to fully explain the reasons for the existence of these phenomena.</p> <p>For example, when the training accuracy is close to 100%, the test accuracy will gradually decrease with pruning. Why does the model not forget the complex features in the data at this time, but overfit the noise more seriously? We also observed that the learning distance of the model will increase first and then decrease with the increase of sparsity. Why does pruning cause such a change in the learning distance of the model? And the double descent phenomenon of deep learning models often needs to add label noise to the input to be observed <d-cite key="NakkiranKBYBS20"></d-cite>, what is the mechanism behind whether double descent occurs?</p> <p>There are still many questions that remain unanswered. We are also now working on a new theoretical work that will shed light on one or more of these issues. I hope that the fog can be cleared as soon as possible to find out the essential reasons behind this phenomenon.</p> <p>PaperÔºö <a href="https:‚Äã//arxiv.org/abs/2206.08684">Sparse Double Descent: Where Network Pruning Aggravates Overfitting</a></p> <p>CodeÔºö <a href="https://github.com/he-zh/sparse-double-descent">‚Äãgithub.com/he-zh/sparse-double-descent</a></p>]]></content><author><name>Zheng HE</name></author><category term="research"/><category term="paper-digest"/><category term="sparse"/><category term="overparameterization"/><summary type="html"><![CDATA[a brief introduction for the ICML paper of sparse double descent]]></summary></entry><entry><title type="html">Overparameterization and sparsity</title><link href="https://he-zh.github.io/blog/2021/overparametrization-sparsity/" rel="alternate" type="text/html" title="Overparameterization and sparsity"/><published>2021-08-30T00:00:00+00:00</published><updated>2021-08-30T00:00:00+00:00</updated><id>https://he-zh.github.io/blog/2021/overparametrization-sparsity</id><content type="html" xml:base="https://he-zh.github.io/blog/2021/overparametrization-sparsity/"><![CDATA[<p>It is a mystery how overparameterized models behave and why. It is especially intriguing why overparameterized models can generalize well despite the excessive capacity, and why highly sparse neural networks can still achieve comparable performance to the dense networks (as suggested in the <a href="https://arxiv.org/abs/1803.03635">lottery ticket hypothesis</a>).</p> <p>Over the past few months, I have been intrigued by the possible relationship between model generalization and sparsity. The slides below give a brief review of several related works and the questions that I cared about. Luckily I am able to answer a few of them with my own research now (see <a href="https://he-zh.github.io/_posts/2022-07-Sparse_double_descent/">sparse double descent</a>).</p> <object data="/assets/pdf/ZhengHe_overparametrized_neural_networks.pdf" width="100%" height="500px"> <p>Unable to display PDF file. <a href="/assets/pdf/ZhengHe_overparametrized_neural_networks.pdf">Download</a> instead.</p> </object>]]></content><author><name></name></author><category term="research"/><category term="survey"/><category term="paper-digest"/><category term="slides"/><category term="sparse"/><category term="overparameterization"/><summary type="html"><![CDATA[a review on recent paper about overparameterization and sparsity]]></summary></entry></feed>